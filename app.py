import streamlit as st
import requests
import pandas as pd
import time
import random
import urllib3
from datetime import datetime
from bs4 import BeautifulSoup

# SSL ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸° (ê¹”ë”í•œ ë¡œê·¸ë¥¼ ìœ„í•´)
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# í˜ì´ì§€ ê¸°ë³¸ ì„¤ì •
st.set_page_config(page_title="Steam ë°ì´í„° ìˆ˜ì§‘ê¸°", layout="wide")

# --- ğŸ” ë¹„ë°€ë²ˆí˜¸ ì ê¸ˆ ---
password = st.text_input("ì ‘ì† ì•”í˜¸", type="password")
if password != "smilegate":
    st.warning("ì•”í˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    st.stop()

st.title("Steam ë°ì´í„° ìˆ˜ì§‘ê¸°")

# --- ì‚¬ì´ë“œë°” ---
with st.sidebar:
    st.header("ì„¤ì •")
    menu = st.selectbox("ë¶„ì„ ì±„ë„", ["Steam (ìŠ¤íŒ€)", "Reddit (ì¤€ë¹„ì¤‘)", "YouTube (ì¤€ë¹„ì¤‘)"])
    st.divider()

if menu == "Steam (ìŠ¤íŒ€)":
    tab1, tab2 = st.tabs(["ë¦¬ë·° ìˆ˜ì§‘ (API)", "í† ë¡ ì¥ ìˆ˜ì§‘ (í¬ë¡¤ë§)"])
    
    # =========================================================
    # [TAB 1] ë¦¬ë·° ìˆ˜ì§‘ (ê³µì‹ API ì‚¬ìš©)
    # =========================================================
    with tab1:
        st.subheader("ë¦¬ë·° ë°ì´í„° ìˆ˜ì§‘")
        
        col1, col2 = st.columns(2)
        with col1:
            app_id_review = st.text_input("App ID (ë¦¬ë·°ìš©)", value="1562700")
        with col2:
            language = st.selectbox("ì–¸ì–´", ["all", "koreana", "english", "japanese", "schinese"], index=0)
        
        start_date = st.date_input("ìˆ˜ì§‘ ì‹œì‘ ë‚ ì§œ", datetime(2025, 2, 1))
        
        if st.button("ë¦¬ë·° ìˆ˜ì§‘ ì‹œì‘", key="btn_review"):
            all_reviews = []
            cursor = '*'
            status_box = st.info(f"ë°ì´í„° ìˆ˜ì§‘ ì¤‘... (ëª©í‘œ: {start_date} ì´í›„)")
            
            try:
                # ìµœëŒ€ 100í˜ì´ì§€ (ì•½ 1ë§Œê°œ) ì•ˆì „ ì¥ì¹˜
                for i in range(100): 
                    params = {
                        'json': 1, 
                        'cursor': cursor, 
                        'language': language,
                        'num_per_page': 100, 
                        'purchase_type': 'all', 
                        'filter': 'recent'
                    }
                    # verify=Falseë¡œ ë³´ì•ˆ ì´ìŠˆ ë°©ì§€
                    res = requests.get(f"https://store.steampowered.com/appreviews/{app_id_review}", params=params, verify=False)
                    data = res.json()
                    
                    if 'reviews' in data and len(data['reviews']) > 0:
                        last_ts = data['reviews'][-1]['timestamp_created']
                        curr_date = pd.to_datetime(last_ts, unit='s').date()
                        
                        for r in data['reviews']:
                            r_date = pd.to_datetime(r['timestamp_created'], unit='s').date()
                            if r_date >= start_date:
                                all_reviews.append({
                                    'ì‘ì„±ì¼': r_date, 
                                    'ë‚´ìš©': r['review'].replace('\n', ' '), 
                                    'ì¶”ì²œìˆ˜': r['votes_up'],
                                    'í”Œë ˆì´ì‹œê°„(ë¶„)': r['author'].get('playtime_forever', 0)
                                })
                        
                        cursor = data['cursor']
                        status_box.info(f"í˜„ì¬ {len(all_reviews)}ê°œ ìˆ˜ì§‘ë¨... (íƒìƒ‰ ë‚ ì§œ: {curr_date})")
                        
                        if curr_date < start_date: break
                    else: break
                
                if all_reviews:
                    df = pd.DataFrame(all_reviews)
                    filtered_df = df[df['ì‘ì„±ì¼'] >= start_date]
                    st.success(f"ì™„ë£Œ! ì´ {len(filtered_df)}ê°œì˜ ë¦¬ë·°ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
                    st.dataframe(filtered_df)
                    st.download_button("ì—‘ì…€ ë‹¤ìš´ë¡œë“œ", filtered_df.to_csv(index=False).encode('utf-8-sig'), "steam_reviews.csv")
                else:
                    st.warning("í•´ë‹¹ ê¸°ê°„ì˜ ë¦¬ë·°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    
            except Exception as e:
                st.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")

    # =========================================================
    # [TAB 2] í† ë¡ ì¥ ìˆ˜ì§‘ (ì„±ê³µí•œ í¬ë¡¤ë§ ë¡œì§ ì ìš©)
    # =========================================================
    with tab2:
        st.subheader("í† ë¡ ì¥ ìƒì„¸ ìˆ˜ì§‘ (ë³¸ë¬¸+ëŒ“ê¸€)")
        
        target_url = st.text_input(
            "ìˆ˜ì§‘í•  í† ë¡ ì¥ URL", 
            value="https://steamcommunity.com/app/1562700/discussions/"
        )
        
        pages_to_crawl = st.number_input("íƒìƒ‰í•  í˜ì´ì§€ ìˆ˜", min_value=1, max_value=20, value=2)
        
        if st.button("í† ë¡ ê¸€ ìˆ˜ì§‘ ì‹œì‘", key="btn_discuss"):
            discussion_data = []
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7'
            }
            cookies = {'wants_mature_content': '1', 'birthtime': '660000001', 'lastagecheckage': '1-January-1990'}
            
            try:
                # URL ì£¼ì†Œ ë³´ì •
                if not target_url.endswith('/') and '?' not in target_url:
                    target_url += '/'

                for p in range(pages_to_crawl):
                    full_url = f"{target_url}?fp={p+1}"
                    status_text.text(f"{p+1}í˜ì´ì§€ ëª©ë¡ì„ ì½ê³  ìˆìŠµë‹ˆë‹¤...")
                    
                    # 1. ëª©ë¡ ì ‘ì†
                    time.sleep(random.uniform(1.0, 2.0))
                    res = requests.get(full_url, headers=headers, cookies=cookies, verify=False, timeout=15)
                    soup = BeautifulSoup(res.text, 'html.parser')
                    
                    # ê²Œì‹œê¸€ ëª©ë¡ ì°¾ê¸° (forum_topic)
                    topic_rows = soup.find_all('div', class_='forum_topic')
                    
                    if not topic_rows:
                        st.warning(f"{p+1}í˜ì´ì§€ì—ì„œ ê¸€ ëª©ë¡ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                        break
                    
                    status_text.text(f"{p+1}í˜ì´ì§€: {len(topic_rows)}ê°œ ê¸€ ë°œê²¬. ìƒì„¸ ë‚´ìš©ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤...")
                    
                    # 2. ìƒì„¸ ë‚´ìš© ìˆ˜ì§‘
                    for idx, row in enumerate(topic_rows):
                        try:
                            # ë§í¬ì™€ ì œëª© ì°¾ê¸°
                            link_tag = row.find('a', class_='forum_topic_overlay')
                            title_tag = row.find('div', class_='forum_topic_name')
                            
                            if not link_tag: continue
                            
                            link = link_tag['href']
                            title = title_tag.text.strip() if title_tag else "ì œëª© ì—†ìŒ"
                            
                            # ìƒì„¸ í˜ì´ì§€ ì ‘ì†
                            time.sleep(random.uniform(0.5, 1.0))
                            sub_res = requests.get(link, headers=headers, cookies=cookies, verify=False)
                            sub_soup = BeautifulSoup(sub_res.text, 'html.parser')
                            
                            # (A) ë³¸ë¬¸ ìˆ˜ì§‘
                            op_div = sub_soup.find('div', class_='forum_op')
                            if op_div:
                                # ì‘ì„±ì (ë¶„ì„ ê²°ê³¼ ë°˜ì˜)
                                author_tag = op_div.find('a', class_='forum_op_author')
                                author = author_tag.text.strip() if author_tag else "Unknown"
                                
                                content_tag = op_div.find('div', class_='content')
                                content = content_tag.text.strip() if content_tag else ""
                                
                                date_tag = op_div.find('span', class_='date')
                                date = date_tag.text.strip() if date_tag else ""
                                
                                discussion_data.append({
                                    'êµ¬ë¶„': 'ê²Œì‹œê¸€', 'ì œëª©': title, 'ì‘ì„±ì': author, 
                                    'ë‚´ìš©': content, 'ì‘ì„±ì¼': date, 'ë§í¬': link
                                })
                            
                            # (B) ëŒ“ê¸€ ìˆ˜ì§‘
                            comments = sub_soup.find_all('div', class_='commentthread_comment')
                            for comm in comments:
                                try:
                                    text_div = comm.find('div', class_='commentthread_comment_text')
                                    c_text = text_div.text.strip() if text_div else ""
                                    
                                    author_div = comm.find('a', class_='commentthread_author_link')
                                    c_author = author_div.text.strip() if author_div else "Unknown"
                                    
                                    if c_text:
                                        discussion_data.append({
                                            'êµ¬ë¶„': 'ëŒ“ê¸€', 'ì œëª©': f"(Re) {title}", 
                                            'ì‘ì„±ì': c_author, 'ë‚´ìš©': c_text, 
                                            'ì‘ì„±ì¼': '-', 'ë§í¬': link
                                        })
                                except: continue

                        except Exception:
                            continue
                        
                        # ì§„í–‰ë¥  ë°” ì—…ë°ì´íŠ¸
                        current_progress = (p / pages_to_crawl) + ((idx + 1) / len(topic_rows) / pages_to_crawl)
                        progress_bar.progress(min(current_progress, 0.99))

                progress_bar.progress(1.0)
                
                if discussion_data:
                    df = pd.DataFrame(discussion_data)
                    st.success(f"ìˆ˜ì§‘ ì™„ë£Œ! ì´ {len(df)}ê°œì˜ ë°ì´í„°(ë³¸ë¬¸+ëŒ“ê¸€)")
                    st.dataframe(df)
                    st.download_button("ì—‘ì…€ ë‹¤ìš´ë¡œë“œ", df.to_csv(index=False).encode('utf-8-sig'), "steam_discussion_final.csv")
                else:
                    st.error("ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

            except Exception as e:
                st.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")