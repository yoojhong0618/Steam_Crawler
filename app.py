import streamlit as st
import requests
import pandas as pd
import time
import random
import urllib3
from bs4 import BeautifulSoup

# SSL ê²½ê³  ë¬´ì‹œ
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

st.set_page_config(page_title="ìŠ¤íŒ€ ì •ë°€ ìˆ˜ì§‘ê¸°", layout="wide")
st.title("ğŸ•·ï¸ ìŠ¤íŒ€ í† ë¡ ì¥ ìˆ˜ì§‘ê¸° (HTML ë¶„ì„ ì™„ë£Œ)")

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.header("ì„¤ì •")
    target_url = st.text_input("ìˆ˜ì§‘í•  í† ë¡ ì¥ URL", value="https://steamcommunity.com/app/1562700/discussions/")
    pages_to_crawl = st.number_input("íƒìƒ‰ í˜ì´ì§€ ìˆ˜", min_value=1, value=2)
    run_btn = st.button("ìˆ˜ì§‘ ì‹œì‘ ğŸš€", type="primary")

if run_btn:
    discussion_data = []
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7'
    }
    cookies = {'wants_mature_content': '1', 'birthtime': '660000001', 'lastagecheckage': '1-January-1990'}
    
    try:
        # URL ì •ë¦¬
        if not target_url.endswith('/') and '?' not in target_url:
            target_url += '/'

        for p in range(pages_to_crawl):
            full_url = f"{target_url}?fp={p+1}"
            status_text.text(f"ğŸ“¡ {p+1}í˜ì´ì§€ ëª©ë¡ ì½ëŠ” ì¤‘...")
            
            # 1. ëª©ë¡ í˜ì´ì§€ ì ‘ì†
            time.sleep(random.uniform(1.0, 2.0))
            res = requests.get(full_url, headers=headers, cookies=cookies, verify=False, timeout=15)
            soup = BeautifulSoup(res.text, 'html.parser')
            
            # ëª©ë¡ ì°¾ê¸° (forum_topic í´ë˜ìŠ¤ ì‚¬ìš©)
            topic_rows = soup.find_all('div', class_='forum_topic')
            
            if not topic_rows:
                st.warning(f"âš ï¸ {p+1}í˜ì´ì§€: ê¸€ ëª©ë¡ì„ ëª» ì°¾ì•˜ìŠµë‹ˆë‹¤.")
                break
            
            status_text.text(f"âœ… {p+1}í˜ì´ì§€: {len(topic_rows)}ê°œ ê¸€ ë°œê²¬! ë‚´ìš©ì„ ê¸ì–´ì˜µë‹ˆë‹¤...")
            
            # 2. ìƒì„¸ í˜ì´ì§€ ë£¨í”„
            for idx, row in enumerate(topic_rows):
                try:
                    # (A) ë§í¬ì™€ ì œëª© ì°¾ê¸°
                    link_tag = row.find('a', class_='forum_topic_overlay')
                    title_tag = row.find('div', class_='forum_topic_name')
                    
                    if not link_tag: continue
                    
                    link = link_tag['href']
                    title = title_tag.text.strip() if title_tag else "ì œëª© ì—†ìŒ"
                    
                    # (B) ìƒì„¸ í˜ì´ì§€ ì ‘ì†
                    time.sleep(random.uniform(0.3, 0.7))
                    sub_res = requests.get(link, headers=headers, cookies=cookies, verify=False)
                    sub_soup = BeautifulSoup(sub_res.text, 'html.parser')
                    
                    # --- [í•µì‹¬ ìˆ˜ì •: ë³´ë‚´ì£¼ì‹  HTML ë¶„ì„ ë°˜ì˜] ---
                    
                    # 1. ë³¸ë¬¸(OP) ìˆ˜ì§‘
                    op_div = sub_soup.find('div', class_='forum_op')
                    if op_div:
                        # ì‘ì„±ì: forum_op_author í´ë˜ìŠ¤ ì•ˆì˜ í…ìŠ¤íŠ¸
                        author_tag = op_div.find('a', class_='forum_op_author')
                        author = author_tag.text.strip() if author_tag else "Unknown"
                        
                        # ë‚´ìš©: content í´ë˜ìŠ¤
                        content_tag = op_div.find('div', class_='content')
                        content = content_tag.text.strip() if content_tag else ""
                        
                        # ë‚ ì§œ: date í´ë˜ìŠ¤
                        date_tag = op_div.find('span', class_='date')
                        date = date_tag.text.strip() if date_tag else ""
                        
                        discussion_data.append({
                            'Type': 'ê²Œì‹œê¸€(ë³¸ë¬¸)', 
                            'Title': title, 
                            'Author': author, 
                            'Content': content, 
                            'Date': date, 
                            'Link': link
                        })
                    
                    # 2. ëŒ“ê¸€(Comments) ìˆ˜ì§‘
                    # commentthread_comment í´ë˜ìŠ¤ë¥¼ ê°€ì§„ ëª¨ë“  divë¥¼ ì°¾ìŒ
                    comments = sub_soup.find_all('div', class_='commentthread_comment')
                    
                    for comm in comments:
                        try:
                            # ëŒ“ê¸€ ë‚´ìš©: commentthread_comment_text í´ë˜ìŠ¤
                            text_div = comm.find('div', class_='commentthread_comment_text')
                            c_text = text_div.text.strip() if text_div else ""
                            
                            # ëŒ“ê¸€ ì‘ì„±ì: commentthread_author_link í´ë˜ìŠ¤
                            author_div = comm.find('a', class_='commentthread_author_link')
                            c_author = author_div.text.strip() if author_div else "Unknown"
                            
                            # ëŒ“ê¸€ ë‚ ì§œ: commentthread_comment_timestamp í´ë˜ìŠ¤
                            date_span = comm.find('span', class_='commentthread_comment_timestamp')
                            c_date = date_span.text.strip() if date_span else "-"

                            # ë‚´ìš©ì´ ìˆì„ ë•Œë§Œ ì €ì¥
                            if c_text:
                                discussion_data.append({
                                    'Type': 'ã„´ëŒ“ê¸€', 
                                    'Title': f"(Re) {title}", 
                                    'Author': c_author, 
                                    'Content': c_text, 
                                    'Date': c_date, 
                                    'Link': link
                                })
                        except:
                            continue # íŠ¹ì • ëŒ“ê¸€ ì—ëŸ¬ë‚˜ë©´ ê±´ë„ˆë›°ê¸°

                except Exception as e:
                    # ê¸€ í•˜ë‚˜ ì—ëŸ¬ë‚˜ë„ ë©ˆì¶”ì§€ ì•ŠìŒ
                    continue
                
                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                current_progress = (p / pages_to_crawl) + ((idx + 1) / len(topic_rows) / pages_to_crawl)
                progress_bar.progress(min(current_progress, 0.99))

        progress_bar.progress(1.0)
        
        if discussion_data:
            df = pd.DataFrame(discussion_data)
            st.success(f"ğŸ‰ ëŒ€ì„±ê³µ! ì´ {len(df)}ê°œì˜ ë°ì´í„°(ë³¸ë¬¸+ëŒ“ê¸€)ë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")
            st.dataframe(df)
            st.download_button("í† ë¡ ì¥ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ", df.to_csv(index=False).encode('utf-8-sig'), "steam_discuss_complete.csv")
        else:
            st.error("ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨. (ëª©ë¡ì€ ì°¾ì•˜ìœ¼ë‚˜ ìƒì„¸ ë‚´ìš©ì„ ëª» ì½ìŒ)")

    except Exception as e:
        st.error(f"ì—ëŸ¬ ë°œìƒ: {e}")