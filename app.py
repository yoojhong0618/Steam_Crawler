import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup
from googleapiclient.discovery import build
from datetime import datetime, time
import time as time_lib

# ==========================================
# 1. ê³µí†µ í•¨ìˆ˜ ë° ì„¤ì •
# ==========================================
st.set_page_config(page_title="Game Community Crawler", layout="wide")
st.title("ğŸ® í†µí•© ê²Œì„ ì—¬ë¡  ë¶„ì„ê¸° (Steam & YouTube)")

# ì‚¬ì´ë“œë°”: API í‚¤ ë° ì„¤ì •
st.sidebar.header("âš™ï¸ ì„¤ì • (Settings)")
youtube_api_key = st.sidebar.text_input("YouTube Data API Key", type="password", help="YouTube ë°ì´í„° ìˆ˜ì§‘ì„ ìœ„í•´ í•„ìˆ˜ì…ë‹ˆë‹¤.")

# ==========================================
# 2. Steam ê´€ë ¨ í•¨ìˆ˜ (ë¦¬ë·° & í† ë¡ ì¥)
# ==========================================

def get_steam_game_id(game_name):
    """ê²Œì„ ì´ë¦„ìœ¼ë¡œ Steam App IDë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    url = "https://store.steampowered.com/search/"
    params = {'term': game_name}
    try:
        response = requests.get(url, params=params)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        search_results = soup.find_all('a', class_='search_result_row')
        
        games = []
        for result in search_results:
            try:
                title = result.find('span', class_='title').text
                app_id = result['data-ds-appid']
                games.append((title, app_id))
            except:
                continue
        return games
    except Exception as e:
        return []

def get_steam_reviews(app_id, language='english', num_reviews=100):
    """íŠ¹ì • ê²Œì„ì˜ ë¦¬ë·°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
    url = f"https://store.steampowered.com/appreviews/{app_id}"
    reviews = []
    cursor = '*'
    
    params = {
        'json': 1,
        'filter': 'updated',
        'language': language,
        'day_range': 9223372036854775807,
        'review_type': 'all',
        'purchase_type': 'all',
        'num_per_page': 100
    }

    while len(reviews) < num_reviews:
        params['cursor'] = cursor
        try:
            response = requests.get(url, params=params)
            data = response.json()
            
            if 'reviews' in data and len(data['reviews']) > 0:
                for r in data['reviews']:
                    reviews.append({
                        'Author': r['author']['steamid'],
                        'Playtime_Forever': r['author']['playtime_forever'],
                        'Review_Text': r['review'],
                        'Voted_Up': r['voted_up'],
                        'Votes_Up': r['votes_up'],
                        'Date_Posted': datetime.fromtimestamp(r['timestamp_created']).strftime('%Y-%m-%d')
                    })
                cursor = data['cursor']
            else:
                break
        except:
            break
            
    return pd.DataFrame(reviews[:num_reviews])

def get_steam_discussions(app_id, max_pages=3):
    """Steam í† ë¡ ì¥ì˜ ì œëª©ê³¼ ë‚´ìš©ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤ (ê°„ì´ í¬ë¡¤ë§)."""
    discussions = []
    base_url = f"https://steamcommunity.com/app/{app_id}/discussions/"
    
    # í† ë¡ ì¥ ëª©ë¡ í˜ì´ì§€ ìˆœíšŒ
    for page in range(1, max_pages + 1):
        try:
            # fp íŒŒë¼ë¯¸í„°ê°€ ì—†ìœ¼ë©´ 1í˜ì´ì§€, ì´í›„ëŠ” Steam ë°©ì‹ì´ ë³µì¡í•˜ì—¬ ë‹¨ìˆœ ì˜ˆì‹œë¡œ 1í˜ì´ì§€ë§Œ í¬ë¡¤ë§í•˜ê±°ë‚˜
            # ì •í™•í•œ í˜ì´ì§€ë„¤ì´ì…˜ì„ ìœ„í•´ì„œëŠ” Seleniumì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
            # ì—¬ê¸°ì„œëŠ” requestsë¡œ ê°€ì¥ ìµœì‹ /ì¸ê¸° í† ë¡ ê¸€ ëª©ë¡(1í˜ì´ì§€)ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
            res = requests.get(base_url)
            soup = BeautifulSoup(res.text, 'html.parser')
            
            rows = soup.find_all('div', class_='forum_topic')
            if not rows: break

            for row in rows:
                topic = row.find('div', class_='forum_topic_name')
                author = row.find('div', class_='forum_topic_op')
                reply_count = row.find('div', class_='forum_topic_reply_count')
                
                if topic:
                    title_text = topic.get_text(strip=True)
                    # ìƒì„¸ ë§í¬
                    link = topic.find('a')['href']
                    
                    discussions.append({
                        "Title": title_text,
                        "Author": author.get_text(strip=True) if author else "Unknown",
                        "Replies": reply_count.get_text(strip=True) if reply_count else "0",
                        "Link": link
                    })
        except Exception as e:
            st.error(f"í† ë¡ ì¥ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
            break
            
    return pd.DataFrame(discussions)

# ==========================================
# 3. YouTube ê´€ë ¨ í•¨ìˆ˜
# ==========================================

def get_youtube_videos(api_key, query, start, end, max_results):
    """ê¸°ê°„ ë‚´ ì˜ìƒ ê²€ìƒ‰"""
    youtube = build('youtube', 'v3', developerKey=api_key)
    
    start_dt = datetime.combine(start, time.min).isoformat() + "Z"
    end_dt = datetime.combine(end, time.max).isoformat() + "Z"
    
    video_list = []
    try:
        search_response = youtube.search().list(
            q=query,
            type="video",
            part="id,snippet",
            order="viewCount",  # ì¡°íšŒìˆ˜ ë†’ì€ ìˆœ ê²€ìƒ‰
            publishedAfter=start_dt,
            publishedBefore=end_dt,
            maxResults=max_results
        ).execute()

        for item in search_response.get("items", []):
            video_list.append({
                "video_id": item["id"]["videoId"],
                "title": item["snippet"]["title"],
                "published_at": item["snippet"]["publishedAt"],
                "channel": item["snippet"]["channelTitle"]
            })
    except Exception as e:
        st.error(f"YouTube ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        
    return video_list

def get_youtube_comments(api_key, video_id, max_comments):
    """ì˜ìƒ ëŒ“ê¸€ ìˆ˜ì§‘ (ì¸ê¸°ìˆœ ì •ë ¬)"""
    youtube = build('youtube', 'v3', developerKey=api_key)
    comments = []
    
    try:
        request = youtube.commentThreads().list(
            part="snippet",
            videoId=video_id,
            maxResults=min(max_comments, 100),
            textFormat="plainText",
            order="relevance"  # â˜… í•µì‹¬: ì¸ê¸° ëŒ“ê¸€ ìˆœ ì •ë ¬ â˜…
        )
        
        while request and len(comments) < max_comments:
            response = request.execute()
            
            for item in response['items']:
                comment_snip = item['snippet']['topLevelComment']['snippet']
                comments.append({
                    "Author": comment_snip['authorDisplayName'],
                    "Comment": comment_snip['textDisplay'],
                    "Likes": comment_snip['likeCount'],
                    "Date": comment_snip['publishedAt']
                })
                
            if 'nextPageToken' in response and len(comments) < max_comments:
                request = youtube.commentThreads().list_next(request, response)
            else:
                break
    except Exception as e:
        pass # ëŒ“ê¸€ ì¤‘ì§€ëœ ì˜ìƒ ë“± ì˜ˆì™¸ ì²˜ë¦¬
        
    return comments

# ==========================================
# 4. ë©”ì¸ UI (íƒ­ êµ¬ì„±)
# ==========================================

tab1, tab2, tab3 = st.tabs(["ğŸŸ¦ Steam Reviews", "ğŸ’¬ Steam Discussions", "ğŸŸ¥ YouTube Crawler"])

# --- Tab 1: Steam ë¦¬ë·° ---
with tab1:
    st.header("Steam ê²Œì„ ë¦¬ë·° ìˆ˜ì§‘")
    game_name_input = st.text_input("ê²Œì„ ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: Elden Ring)", key="steam_review_search")
    
    if game_name_input:
        games = get_steam_game_id(game_name_input)
        if games:
            game_options = {name: app_id for name, app_id in games}
            selected_game = st.selectbox("ê²Œì„ì„ ì„ íƒí•˜ì„¸ìš”", list(game_options.keys()), key="review_select")
            app_id = game_options[selected_game]
            
            num_reviews = st.slider("ìˆ˜ì§‘í•  ë¦¬ë·° ê°œìˆ˜", 10, 1000, 100, step=10, key="review_slider")
            
            if st.button("ë¦¬ë·° ìˆ˜ì§‘ ì‹œì‘", key="btn_review"):
                with st.spinner("ë¦¬ë·°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘..."):
                    df_reviews = get_steam_reviews(app_id, num_reviews=num_reviews)
                    if not df_reviews.empty:
                        st.success(f"{len(df_reviews)}ê°œì˜ ë¦¬ë·°ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤!")
                        st.dataframe(df_reviews)
                        
                        csv = df_reviews.to_csv(index=False).encode('utf-8-sig')
                        st.download_button("CSV ë‹¤ìš´ë¡œë“œ", csv, f"steam_reviews_{app_id}.csv", "text/csv")
                    else:
                        st.warning("ë¦¬ë·°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        else:
            st.warning("ê²Œì„ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# --- Tab 2: Steam í† ë¡ ì¥ ---
with tab2:
    st.header("Steam í† ë¡ ì¥ ì£¼ì œ ìˆ˜ì§‘")
    st.info("í˜„ì¬ í™œì„±í™”ëœ í† ë¡  ì£¼ì œ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.")
    
    # ìœ„ì™€ ë™ì¼í•œ ê²€ìƒ‰ ë¡œì§ ì‚¬ìš©
    game_name_input_disc = st.text_input("ê²Œì„ ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš”", key="steam_disc_search")
    
    if game_name_input_disc:
        games = get_steam_game_id(game_name_input_disc)
        if games:
            game_options = {name: app_id for name, app_id in games}
            selected_game_disc = st.selectbox("ê²Œì„ì„ ì„ íƒí•˜ì„¸ìš”", list(game_options.keys()), key="disc_select")
            app_id_disc = game_options[selected_game_disc]
            
            if st.button("í† ë¡ ì¥ ê¸€ ëª©ë¡ ìˆ˜ì§‘", key="btn_disc"):
                with st.spinner("í† ë¡ ì¥ì„ ê²€ìƒ‰ ì¤‘ì…ë‹ˆë‹¤..."):
                    df_disc = get_steam_discussions(app_id_disc)
                    if not df_disc.empty:
                        st.success(f"í˜„ì¬ í˜ì´ì§€ì˜ í† ë¡  ê¸€ {len(df_disc)}ê°œë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")
                        st.dataframe(df_disc)
                        
                        # ë§í¬ í´ë¦­ ê°€ëŠ¥í•˜ê²Œ ë§Œë“¤ê¸° (ì„ íƒ ì‚¬í•­)
                        for index, row in df_disc.iterrows():
                            st.markdown(f"**[{row['Title']}]({row['Link']})** - ì‘ì„±ì: {row['Author']} (ëŒ“ê¸€: {row['Replies']})")
                    else:
                        st.warning("í† ë¡  ê¸€ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

# --- Tab 3: YouTube ---
with tab3:
    st.header("YouTube ì˜ìƒ ë° ì¸ê¸° ëŒ“ê¸€ ìˆ˜ì§‘")
    
    if not youtube_api_key:
        st.warning("âš ï¸ ì‚¬ì´ë“œë°”ì— YouTube API Keyë¥¼ ë¨¼ì € ì…ë ¥í•´ì£¼ì„¸ìš”!")
    else:
        col1, col2 = st.columns(2)
        with col1:
            yt_query = st.text_input("ê²€ìƒ‰ì–´ (ê²Œì„ ì´ë¦„)", "League of Legends")
            start_date = st.date_input("ì‹œì‘ ë‚ ì§œ", value=datetime(2024, 1, 1))
        with col2:
            max_vids = st.slider("ìµœëŒ€ ì˜ìƒ ê°œìˆ˜", 5, 50, 10)
            end_date = st.date_input("ì¢…ë£Œ ë‚ ì§œ", value=datetime.now())
            
        st.caption("â€» ëŒ“ê¸€ì€ 'ì¸ê¸°ìˆœ(Relevance)'ìœ¼ë¡œ ì •ë ¬ë˜ì–´ ìˆ˜ì§‘ë©ë‹ˆë‹¤.")

        if st.button("YouTube ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘", key="btn_yt"):
            with st.status("ë°ì´í„° ìˆ˜ì§‘ ì§„í–‰ ì¤‘...", expanded=True) as status:
                st.write("ğŸ” ì˜ìƒì„ ê²€ìƒ‰í•©ë‹ˆë‹¤...")
                videos = get_youtube_videos(youtube_api_key, yt_query, start_date, end_date, max_vids)
                
                if not videos:
                    status.update(label="ì˜ìƒì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.", state="error")
                else:
                    st.write(f"ì´ {len(videos)}ê°œì˜ ì˜ìƒì„ ì°¾ì•˜ìŠµë‹ˆë‹¤. ëŒ“ê¸€ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
                    
                    all_yt_data = []
                    progress_bar = st.progress(0)
                    
                    for idx, video in enumerate(videos):
                        progress_bar.progress((idx + 1) / len(videos))
                        # ì˜ìƒ ì •ë³´ í‘œì‹œ
                        st.write(f"ğŸ“º Processing: {video['title'][:30]}...")
                        
                        # ëŒ“ê¸€ ìˆ˜ì§‘ (ì˜ìƒë‹¹ ìµœëŒ€ 50ê°œ ì œí•œ ì˜ˆì‹œ)
                        comments = get_youtube_comments(youtube_api_key, video['video_id'], 50)
                        
                        for c in comments:
                            all_yt_data.append({
                                "Video_Title": video['title'],
                                "Video_Channel": video['channel'],
                                "Video_Publish_Date": video['published_at'],
                                "Comment_Author": c['Author'],
                                "Comment_Text": c['Comment'],
                                "Comment_Likes": c['Likes'],
                                "Comment_Date": c['Date']
                            })
                        time_lib.sleep(0.1) # API ë¶€í•˜ ë°©ì§€
                    
                    status.update(label="ìˆ˜ì§‘ ì™„ë£Œ!", state="complete")
                    
                    if all_yt_data:
                        df_yt = pd.DataFrame(all_yt_data)
                        st.success(f"ì´ {len(df_yt)}ê°œì˜ ëŒ“ê¸€ì„ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
                        st.dataframe(df_yt.head())
                        
                        csv_yt = df_yt.to_csv(index=False).encode('utf-8-sig')
                        st.download_button(
                            label="CSV ë‹¤ìš´ë¡œë“œ (YouTube)",
                            data=csv_yt,
                            file_name=f"youtube_{yt_query}_comments.csv",
                            mime="text/csv"
                        )
                    else:
                        st.warning("ìˆ˜ì§‘ëœ ëŒ“ê¸€ì´ ì—†ìŠµë‹ˆë‹¤.")