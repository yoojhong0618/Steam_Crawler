import streamlit as st
import requests
import pandas as pd
import time
from datetime import datetime, timedelta
from bs4 import BeautifulSoup

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="í†µí•© ê²Œì„ ì—¬ë¡  ë¶„ì„ê¸°", layout="wide")

# --- ğŸ” ë¹„ë°€ë²ˆí˜¸ ì ê¸ˆ ---
password = st.text_input("ğŸ”’ ì ‘ì† ì•”í˜¸", type="password")
if password != "smilegate":
    st.warning("ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤.")
    st.stop()

# --- ì‚¬ì´ë“œë°” ---
with st.sidebar:
    st.title("ğŸ•¹ï¸ Smilegate Research")
    
    menu = st.selectbox(
        "ë¶„ì„ ì±„ë„ ì„ íƒ", 
        ["Steam (ìŠ¤íŒ€)", "Reddit (ë ˆë”§ - ì¤€ë¹„ì¤‘)", "YouTube (ìœ íŠœë¸Œ - ì¤€ë¹„ì¤‘)"]
    )
    st.divider()

# =========================================================
# ğŸ® 1. Steam ë¡œì§
# =========================================================
if menu == "Steam (ìŠ¤íŒ€)":
    st.header("ğŸ® Steam ë°ì´í„° ìˆ˜ì§‘")
    
    tab1, tab2 = st.tabs(["â­ ë¦¬ë·°(Review) ìˆ˜ì§‘", "ğŸ—£ï¸ í† ë¡ ì¥(Discussion) ìˆ˜ì§‘"])
    
    # -----------------------------------------------------
    # [TAB 1] ë¦¬ë·° ìˆ˜ì§‘ê¸° (ê¸°ì¡´ ì™„ë²½ ì½”ë“œ)
    # -----------------------------------------------------
    with tab1:
        col1, col2 = st.columns(2)
        with col1:
            app_id_review = st.text_input("App ID (ë¦¬ë·°ìš©)", value="1562700")
        with col2:
            language = st.selectbox("ì–¸ì–´", ["all", "koreana", "english", "japanese", "schinese"], index=0)
            
        start_date = st.date_input("ì‹œì‘ì¼", datetime(2025, 2, 1))
        
        if st.button("ë¦¬ë·° ìˆ˜ì§‘ ì‹œì‘ ğŸš€", key="btn_review"):
            st.toast("ë¦¬ë·° íƒìƒ‰ ì‹œì‘...")
            all_reviews = []
            cursor = '*'
            progress_bar = st.progress(0)
            status_box = st.info(f"íƒìƒ‰ ì¤‘... (ëª©í‘œ: {start_date})")
            
            try:
                for i in range(5000): # 50ë§Œê°œ ì œí•œ
                    params = {
                        'json': 1, 'cursor': cursor, 'language': language,
                        'num_per_page': 100, 'purchase_type': 'all', 'filter': 'recent'
                    }
                    res = requests.get(f"https://store.steampowered.com/appreviews/{app_id_review}", params=params)
                    data = res.json()
                    
                    if 'reviews' in data and len(data['reviews']) > 0:
                        last_ts = data['reviews'][-1]['timestamp_created']
                        curr_date = pd.to_datetime(last_ts, unit='s').date()
                        
                        for r in data['reviews']:
                            r_date = pd.to_datetime(r['timestamp_created'], unit='s').date()
                            all_reviews.append({
                                'ì‘ì„±ì¼': r_date,
                                'ë‚´ìš©': r['review'].replace('\n', ' '),
                                'ì¶”ì²œìˆ˜': r['votes_up'],
                                'í”Œë ˆì´ì‹œê°„(ë¶„)': r['author']['playtime_forever']
                            })
                        cursor = data['cursor']
                        status_box.info(f"{len(all_reviews)}ê°œ ìˆ˜ì§‘ ì¤‘... (í˜„ì¬: {curr_date})")
                        if curr_date < start_date: break
                    else: break
                
                if all_reviews:
                    df = pd.DataFrame(all_reviews)
                    mask = (df['ì‘ì„±ì¼'] >= start_date)
                    filtered_df = df.loc[mask]
                    st.success(f"{len(filtered_df)}ê°œ ë¦¬ë·° ìˆ˜ì§‘ ì™„ë£Œ!")
                    st.dataframe(filtered_df)
                    st.download_button("ì—‘ì…€ ë‹¤ìš´ë¡œë“œ", filtered_df.to_csv(index=False).encode('utf-8-sig'), "steam_reviews.csv")
            except Exception as e:
                st.error(f"ì—ëŸ¬: {e}")

    # -----------------------------------------------------
    # [TAB 2] í† ë¡ ì¥ ìˆ˜ì§‘ê¸° (ì•ˆì „ì¥ì¹˜ ì¶”ê°€ë¨ âœ¨)
    # -----------------------------------------------------
    with tab2:
        st.info("ğŸ’¡ í† ë¡ ì¥ì€ ì§ì ‘ í˜ì´ì§€ë¥¼ ë°©ë¬¸í•˜ì—¬ ìˆ˜ì§‘í•©ë‹ˆë‹¤. (1í˜ì´ì§€ = ê²Œì‹œê¸€ 15ê°œ + ëŒ“ê¸€ë“¤)")
        
        col_t1, col_t2 = st.columns(2)
        with col_t1:
            app_id_discuss = st.text_input("App ID (í† ë¡ ì¥ìš©)", value="1562700")
        with col_t2:
            # ë„‰ë„‰í•˜ê²Œ 10í˜ì´ì§€ ì…ë ¥í•´ë„, ì—†ìœ¼ë©´ ì•Œì•„ì„œ ë©ˆì¶¥ë‹ˆë‹¤.
            pages_to_crawl = st.number_input("ìµœëŒ€ íƒìƒ‰í•  í˜ì´ì§€ ìˆ˜", min_value=1, max_value=50, value=3)
        
        if st.button("í† ë¡ ê¸€ ìˆ˜ì§‘ ì‹œì‘ ğŸ•µï¸â€â™€ï¸", key="btn_discuss"):
            st.toast("í† ë¡ ì¥ ë°©ë¬¸ ì¤‘...")
            
            discussion_data = []
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            try:
                for p in range(pages_to_crawl):
                    url = f"https://steamcommunity.com/app/{app_id_discuss}/discussions/0/?fp={p+1}"
                    res = requests.get(url)
                    soup = BeautifulSoup(res.text, 'html.parser')
                    
                    # ê²Œì‹œê¸€ ì°¾ê¸°
                    topics = soup.find_all('a', class_='forum_topic_link')
                    
                    # ğŸ‘‡ [í•µì‹¬] ì—¬ê¸°ê°€ ë°”ë¡œ ì•ˆì „ì¥ì¹˜ì…ë‹ˆë‹¤! 
                    # ë§Œì•½ ê²Œì‹œê¸€ì´ í•˜ë‚˜ë„ ì—†ë‹¤ë©´? (í˜ì´ì§€ê°€ ëë‚¬ë‹¤ëŠ” ëœ»)
                    if len(topics) == 0:
                        st.success(f"âœ… {p+1}í˜ì´ì§€ì—ëŠ” ê¸€ì´ ì—†ì–´ì„œ ìˆ˜ì§‘ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. (ì‹¤ì œ í˜ì´ì§€ ë ë„ë‹¬)")
                        progress_bar.progress(100)
                        break 
                    
                    status_text.text(f"ğŸ“„ {p+1}í˜ì´ì§€ ìˆ˜ì§‘ ì¤‘... ({len(topics)}ê°œ ê¸€ ë°œê²¬)")
                    
                    for idx, topic in enumerate(topics):
                        title = topic.text.strip()
                        link = topic['href']
                        
                        # ìƒì„¸ ë‚´ìš© ìˆ˜ì§‘
                        sub_res = requests.get(link)
                        sub_soup = BeautifulSoup(sub_res.text, 'html.parser')
                        
                        content_div = sub_soup.find('div', class_='forum_op')
                        if content_div:
                            author = content_div.find('div', class_='author').text.strip()
                            main_text = content_div.find('div', class_='content').text.strip()
                            date_posted = content_div.find('div', class_='date').text.strip()
                            
                            discussion_data.append({
                                'êµ¬ë¶„': 'ê²Œì‹œê¸€(ë³¸ë¬¸)',
                                'ì œëª©': title,
                                'ì‘ì„±ì': author,
                                'ë‚´ìš©': main_text,
                                'ì‘ì„±ì¼': date_posted,
                                'ë§í¬': link
                            })
                            
                            # ëŒ“ê¸€ ìˆ˜ì§‘
                            comments = sub_soup.find_all('div', class_='commentthread_comment')
                            for comm in comments:
                                try:
                                    c_author = comm.find('bdi').text.strip()
                                    c_text = comm.find('div', class_='commentthread_comment_text').text.strip()
                                    
                                    discussion_data.append({
                                        'êµ¬ë¶„': 'ã„´ëŒ“ê¸€',
                                        'ì œëª©': '-',
                                        'ì‘ì„±ì': c_author,
                                        'ë‚´ìš©': c_text,
                                        'ì‘ì„±ì¼': '-',
                                        'ë§í¬': link
                                    })
                                except: continue
                        
                        time.sleep(0.5) 
                    
                    progress_bar.progress((p + 1) / pages_to_crawl)
                
                if discussion_data:
                    df_discuss = pd.DataFrame(discussion_data)
                    st.divider()
                    st.success(f"ìˆ˜ì§‘ ì™„ë£Œ! ì´ {len(df_discuss)}ê°œì˜ ë°ì´í„°ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
                    st.dataframe(df_discuss)
                    st.download_button("í† ë¡ ì¥ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ", df_discuss.to_csv(index=False).encode('utf-8-sig'), "steam_discussions.csv")
                else:
                    st.warning("ìˆ˜ì§‘ëœ ê¸€ì´ ì—†ìŠµë‹ˆë‹¤.")

            except Exception as e:
                st.error(f"ì˜¤ë¥˜: {e}")

# =========================================================
# ğŸ‘½ 2. Reddit (UIë§Œ ìœ ì§€)
# =========================================================
elif menu == "Reddit (ë ˆë”§ - ì¤€ë¹„ì¤‘)":
    st.header("ğŸ‘½ Reddit ë°ì´í„° ìˆ˜ì§‘")
    st.info("API Keyê°€ ì¤€ë¹„ë˜ë©´ ì½”ë“œë¥¼ ì¶”ê°€í•  ì˜ˆì •ì…ë‹ˆë‹¤.")

# =========================================================
# ğŸ“º 3. YouTube (UIë§Œ ìœ ì§€)
# =========================================================
elif menu == "YouTube (ìœ íŠœë¸Œ - ì¤€ë¹„ì¤‘)":
    st.header("ğŸ“º YouTube ë°ì´í„° ìˆ˜ì§‘")
    st.info("API Keyê°€ ì¤€ë¹„ë˜ë©´ ì½”ë“œë¥¼ ì¶”ê°€í•  ì˜ˆì •ì…ë‹ˆë‹¤.")