import streamlit as st
import requests
import pandas as pd
import time
import random
import urllib3
from bs4 import BeautifulSoup

# SSL ê²½ê³  ë¬´ì‹œ
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

st.set_page_config(page_title="ìŠ¤íŒ€ ìˆ˜ì§‘ê¸° (Final Fix)", layout="wide")

st.title("ğŸ•·ï¸ ìŠ¤íŒ€ í† ë¡ ì¥ ìˆ˜ì§‘ê¸° (êµ¬ì¡° ë³€ê²½ ëŒ€ì‘íŒ)")

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.header("ì„¤ì •")
    target_url = st.text_input("ìˆ˜ì§‘í•  í† ë¡ ì¥ URL", value="https://steamcommunity.com/app/1562700/discussions/")
    pages_to_crawl = st.number_input("íƒìƒ‰ í˜ì´ì§€ ìˆ˜", min_value=1, value=3)
    run_btn = st.button("ìˆ˜ì§‘ ì‹œì‘ ğŸš€", type="primary")

if run_btn:
    st.toast("ìŠ¤íŒ€ ì„œë²„ì— ì ‘ì†í•©ë‹ˆë‹¤...")
    
    discussion_data = []
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7'
    }
    cookies = {'wants_mature_content': '1', 'birthtime': '660000001', 'lastagecheckage': '1-January-1990'}
    
    try:
        # URL ì •ë¦¬
        if not target_url.endswith('/') and '?' not in target_url:
            target_url += '/'

        for p in range(pages_to_crawl):
            full_url = f"{target_url}?fp={p+1}"
            
            # 1. ëª©ë¡ í˜ì´ì§€ ì ‘ì†
            res = requests.get(full_url, headers=headers, cookies=cookies, verify=False, timeout=15)
            soup = BeautifulSoup(res.text, 'html.parser')
            
            # ğŸ” [ìˆ˜ì •ëœ ë¶€ë¶„] class="forum_topic" ì¸ ë©ì–´ë¦¬ë¥¼ ë¨¼ì € ì°¾ìŠµë‹ˆë‹¤.
            topic_divs = soup.find_all('div', class_='forum_topic')
            
            if not topic_divs:
                st.warning(f"âš ï¸ {p+1}í˜ì´ì§€: ê¸€ì„ ëª» ì°¾ì•˜ìŠµë‹ˆë‹¤.")
                # í˜¹ì‹œ ëª¨ë¥´ë‹ˆ HTML êµ¬ì¡° íŒíŠ¸ ë‚¨ê¸°ê¸°
                with st.expander("HTML êµ¬ì¡° í™•ì¸"):
                    st.code(soup.prettify()[:1000], language='html')
                break
            
            status_text.text(f"âœ… {p+1}í˜ì´ì§€: {len(topic_divs)}ê°œ ê¸€ ë°œê²¬! ìƒì„¸ ë‚´ìš©ì„ ì½ìŠµë‹ˆë‹¤...")
            
            # 2. ìƒì„¸ ìˆ˜ì§‘ ë£¨í”„
            for idx, div in enumerate(topic_divs):
                try:
                    # (A) ë§í¬ ì°¾ê¸° (overlay í´ë˜ìŠ¤ì—ì„œ href ì¶”ì¶œ)
                    overlay_link = div.find('a', class_='forum_topic_overlay')
                    if overlay_link:
                        link = overlay_link['href']
                    else:
                        continue # ë§í¬ ì—†ìœ¼ë©´ íŒ¨ìŠ¤

                    # (B) ì œëª© ì°¾ê¸° (topic_name í´ë˜ìŠ¤ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ)
                    name_div = div.find('div', class_='forum_topic_name')
                    title = name_div.text.strip() if name_div else "ì œëª© ì—†ìŒ"
                    
                    # (C) ìƒì„¸ í˜ì´ì§€ ì ‘ì† (Deep Dive)
                    time.sleep(random.uniform(0.3, 0.8)) # ë”œë ˆì´
                    
                    sub_res = requests.get(link, headers=headers, cookies=cookies, verify=False)
                    sub_soup = BeautifulSoup(sub_res.text, 'html.parser')
                    
                    # ë³¸ë¬¸ ë‚´ìš©
                    content_div = sub_soup.find('div', class_='forum_op')
                    if content_div:
                        author = content_div.find('div', class_='author').text.strip()
                        main_text = content_div.find('div', class_='content').text.strip()
                        date_posted = content_div.find('div', class_='date').text.strip()
                        
                        discussion_data.append({
                            'Type': 'ë³¸ë¬¸', 
                            'Title': title, 
                            'Author': author, 
                            'Content': main_text, 
                            'Date': date_posted, 
                            'Link': link
                        })
                        
                        # ëŒ“ê¸€ ë‚´ìš©
                        comments = sub_soup.find_all('div', class_='commentthread_comment')
                        for comm in comments:
                            try:
                                c_author = comm.find('bdi').text.strip()
                                c_text = comm.find('div', class_='commentthread_comment_text').text.strip()
                                discussion_data.append({
                                    'Type': 'ëŒ“ê¸€', 
                                    'Title': f"(Re) {title}", 
                                    'Author': c_author, 
                                    'Content': c_text, 
                                    'Date': '-', 
                                    'Link': link
                                })
                            except: continue

                except Exception as e:
                    print(f"ê¸€ íŒŒì‹± ì—ëŸ¬: {e}")
                    continue
                
                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                current_progress = (p / pages_to_crawl) + ((idx + 1) / len(topic_divs) / pages_to_crawl)
                progress_bar.progress(min(current_progress, 0.99))

        progress_bar.progress(1.0)
        
        if discussion_data:
            df = pd.DataFrame(discussion_data)
            st.success(f"ğŸ‰ ì„±ê³µ! ì´ {len(df)}ê°œì˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
            st.dataframe(df)
            st.download_button("ì—‘ì…€ ë‹¤ìš´ë¡œë“œ", df.to_csv(index=False).encode('utf-8-sig'), "steam_final_data.csv")
        else:
            st.error("ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        st.error(f"ì—ëŸ¬ ë°œìƒ: {e}")