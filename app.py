import streamlit as st
import requests
import pandas as pd
import time
import random
import urllib3
from datetime import datetime, time as dt_time
from bs4 import BeautifulSoup
from googleapiclient.discovery import build

# SSL ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# í˜ì´ì§€ ê¸°ë³¸ ì„¤ì •
st.set_page_config(page_title="Steam & YouTube ë°ì´í„° ìˆ˜ì§‘ê¸°", layout="wide")

# --- ğŸ” ë¹„ë°€ë²ˆí˜¸ ì ê¸ˆ ---
password = st.text_input("ì ‘ì† ì•”í˜¸", type="password")
if password != "smilegate":
    st.warning("ì•”í˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    st.stop()

st.title("Steam & YouTube ë°ì´í„° ìˆ˜ì§‘ê¸°")

# --- ì‚¬ì´ë“œë°” ---
with st.sidebar:
    st.header("ì„¤ì •")
    menu = st.selectbox("ë¶„ì„ ì±„ë„", ["Steam (ìŠ¤íŒ€)", "YouTube (ìœ íŠœë¸Œ)", "Reddit (ì¤€ë¹„ì¤‘)"])
    st.divider()

# =========================================================
# [SECTION 1] Steam (ìŠ¤íŒ€) - ê¸°ì¡´ ì½”ë“œ ìœ ì§€
# =========================================================
if menu == "Steam (ìŠ¤íŒ€)":
    tab1, tab2 = st.tabs(["ë¦¬ë·° ìˆ˜ì§‘ (API)", "í† ë¡ ì¥ ìˆ˜ì§‘ (í¬ë¡¤ë§)"])
    
    # [TAB 1] ë¦¬ë·° ìˆ˜ì§‘
    with tab1:
        st.subheader("ë¦¬ë·° ë°ì´í„° ìˆ˜ì§‘")
        col1, col2 = st.columns(2)
        with col1:
            app_id_review = st.text_input("App ID (ë¦¬ë·°ìš©)", value="1562700")
        with col2:
            language = st.selectbox("ì–¸ì–´", ["all", "koreana", "english", "japanese", "schinese"], index=0)
        
        col_start, col_end = st.columns(2)
        with col_start:
            start_date = st.date_input("ìˆ˜ì§‘ ì‹œì‘ ë‚ ì§œ", datetime(2024, 1, 1))
        with col_end:
            end_date = st.date_input("ìˆ˜ì§‘ ì¢…ë£Œ ë‚ ì§œ", datetime.now())
        
        if st.button("ë¦¬ë·° ìˆ˜ì§‘ ì‹œì‘", key="btn_review"):
            all_reviews = []
            cursor = '*'
            status_box = st.info(f"ë°ì´í„° ìˆ˜ì§‘ ì¤‘... (ëª©í‘œ ê¸°ê°„: {start_date} ~ {end_date})")
            
            try:
                for i in range(200): 
                    params = {
                        'json': 1, 'cursor': cursor, 'language': language,
                        'num_per_page': 100, 'purchase_type': 'all', 'filter': 'recent'
                    }
                    res = requests.get(f"https://store.steampowered.com/appreviews/{app_id_review}", params=params, verify=False)
                    data = res.json()
                    
                    if 'reviews' in data and len(data['reviews']) > 0:
                        last_ts = data['reviews'][-1]['timestamp_created']
                        curr_date = pd.to_datetime(last_ts, unit='s').date()
                        
                        for r in data['reviews']:
                            r_date = pd.to_datetime(r['timestamp_created'], unit='s').date()
                            if r_date > end_date: continue
                            if r_date < start_date: pass 
                            
                            if start_date <= r_date <= end_date:
                                all_reviews.append({
                                    'ì‘ì„±ì¼': r_date, 
                                    'ë‚´ìš©': r['review'].replace('\n', ' '), 
                                    'ì¶”ì²œìˆ˜': r['votes_up'],
                                    'í”Œë ˆì´ì‹œê°„(ë¶„)': r['author'].get('playtime_forever', 0)
                                })
                        
                        cursor = data['cursor']
                        status_box.info(f"í˜„ì¬ {len(all_reviews)}ê°œ ìˆ˜ì§‘ë¨... (í˜„ì¬ íƒìƒ‰ ìœ„ì¹˜: {curr_date})")
                        
                        if curr_date < start_date: break
                    else: break
                
                if all_reviews:
                    df = pd.DataFrame(all_reviews)
                    df = df.sort_values(by='ì‘ì„±ì¼', ascending=False)
                    st.success(f"ì™„ë£Œ! {start_date} ~ {end_date} ê¸°ê°„ì˜ ë¦¬ë·° {len(df)}ê°œë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
                    st.dataframe(df)
                    st.download_button("ì—‘ì…€ ë‹¤ìš´ë¡œë“œ", df.to_csv(index=False).encode('utf-8-sig'), "steam_reviews.csv")
                else:
                    st.warning("í•´ë‹¹ ê¸°ê°„ì— ì‘ì„±ëœ ë¦¬ë·°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            except Exception as e:
                st.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")

    # [TAB 2] í† ë¡ ì¥ ìˆ˜ì§‘
    with tab2:
        st.subheader("í† ë¡ ì¥ ìƒì„¸ ìˆ˜ì§‘ (ë³¸ë¬¸+ëŒ“ê¸€)")
        st.caption("â€» í† ë¡ ì¥ì€ ì›¹ í¬ë¡¤ë§ ë°©ì‹ì´ë¼ 'í˜ì´ì§€ ìˆ˜'ë¡œë§Œ ë²”ìœ„ë¥¼ ì§€ì •í•©ë‹ˆë‹¤.")
        target_url = st.text_input("ìˆ˜ì§‘í•  í† ë¡ ì¥ URL", value="https://steamcommunity.com/app/1562700/discussions/")
        pages_to_crawl = st.number_input("íƒìƒ‰í•  í˜ì´ì§€ ìˆ˜", min_value=1, max_value=20, value=2)
        
        if st.button("í† ë¡ ê¸€ ìˆ˜ì§‘ ì‹œì‘", key="btn_discuss"):
            discussion_data = []
            progress_bar = st.progress(0)
            status_text = st.empty()
            headers = {'User-Agent': 'Mozilla/5.0', 'Accept-Language': 'ko-KR'}
            cookies = {'wants_mature_content': '1', 'birthtime': '660000001', 'lastagecheckage': '1-January-1990'}
            
            try:
                if not target_url.endswith('/') and '?' not in target_url: target_url += '/'
                for p in range(pages_to_crawl):
                    full_url = f"{target_url}?fp={p+1}"
                    status_text.text(f"{p+1}í˜ì´ì§€ ìˆ˜ì§‘ ì¤‘...")
                    time.sleep(1)
                    res = requests.get(full_url, headers=headers, cookies=cookies, verify=False)
                    soup = BeautifulSoup(res.text, 'html.parser')
                    topic_rows = soup.find_all('div', class_='forum_topic')
                    
                    if not topic_rows: break
                    
                    for idx, row in enumerate(topic_rows):
                        try:
                            link_tag = row.find('a', class_='forum_topic_overlay')
                            title_tag = row.find('div', class_='forum_topic_name')
                            if not link_tag: continue
                            link = link_tag['href']
                            title = title_tag.text.strip() if title_tag else "ì œëª© ì—†ìŒ"
                            
                            time.sleep(0.5)
                            sub_res = requests.get(link, headers=headers, cookies=cookies, verify=False)
                            sub_soup = BeautifulSoup(sub_res.text, 'html.parser')
                            
                            op_div = sub_soup.find('div', class_='forum_op')
                            if op_div:
                                author = op_div.find('a', class_='forum_op_author').text.strip()
                                content = op_div.find('div', class_='content').text.strip()
                                discussion_data.append({'êµ¬ë¶„': 'ê²Œì‹œê¸€', 'ì œëª©': title, 'ì‘ì„±ì': author, 'ë‚´ìš©': content, 'ë§í¬': link})
                            
                            comments = sub_soup.find_all('div', class_='commentthread_comment')
                            for comm in comments:
                                c_text = comm.find('div', class_='commentthread_comment_text').text.strip()
                                c_author = comm.find('a', class_='commentthread_author_link').text.strip()
                                if c_text:
                                    discussion_data.append({'êµ¬ë¶„': 'ëŒ“ê¸€', 'ì œëª©': f"(Re) {title}", 'ì‘ì„±ì': c_author, 'ë‚´ìš©': c_text, 'ë§í¬': link})
                        except: continue
                        progress_bar.progress(min((p / pages_to_crawl) + ((idx + 1) / len(topic_rows) / pages_to_crawl), 0.99))
                
                progress_bar.progress(1.0)
                if discussion_data:
                    df = pd.DataFrame(discussion_data)
                    st.success(f"ìˆ˜ì§‘ ì™„ë£Œ! ì´ {len(df)}ê°œ")
                    st.dataframe(df)
                    st.download_button("ì—‘ì…€ ë‹¤ìš´ë¡œë“œ", df.to_csv(index=False).encode('utf-8-sig'), "steam_discussion.csv")
                else: st.error("ë°ì´í„° ì—†ìŒ")
            except Exception as e: st.error(f"ì˜¤ë¥˜: {e}")

# =========================================================
# [SECTION 2] YouTube (ìœ íŠœë¸Œ) - [êµ¬ì¡° ë³€ê²½ë¨]
# =========================================================
elif menu == "YouTube (ìœ íŠœë¸Œ)":
    st.subheader("ğŸŸ¥ YouTube ë°ì´í„° ìˆ˜ì§‘ê¸°")
    
    # API í‚¤ëŠ” ë‘ íƒ­ì—ì„œ ê³µí†µìœ¼ë¡œ ì“°ë¯€ë¡œ ë§¨ ìœ„ë¡œ ëºŒ
    yt_api_key = st.text_input("YouTube Data API Key", type="password")

    # íƒ­ ë¶„ë¦¬: í‚¤ì›Œë“œ ê²€ìƒ‰ vs ê°œë³„ ë§í¬
    tab_yt1, tab_yt2 = st.tabs(["ğŸ” í‚¤ì›Œë“œ ê²€ìƒ‰ (ë‹¤ìˆ˜ ì˜ìƒ)", "ğŸ”— ê°œë³„ ì˜ìƒ ë§í¬ (1ê°œ)"])

    # [TAB 1] ê¸°ì¡´ ê¸°ëŠ¥: í‚¤ì›Œë“œ ê²€ìƒ‰
    with tab_yt1:
        st.caption("íŠ¹ì • í‚¤ì›Œë“œ(ê²Œì„ëª… ë“±)ë¥¼ ê²€ìƒ‰í•˜ì—¬, ì¡°íšŒìˆ˜ê°€ ë†’ì€ ì˜ìƒë“¤ì˜ ëŒ“ê¸€ì„ í•œêº¼ë²ˆì— ìˆ˜ì§‘í•©ë‹ˆë‹¤.")
        
        col1, col2 = st.columns([3, 1])
        with col1:
            search_keyword = st.text_input("ê²€ìƒ‰ì–´ (ì˜ˆ: Elden Ring Review)", value="Elden Ring")
        with col2:
            max_videos = st.number_input("ë¶„ì„í•  ì˜ìƒ ìˆ˜", min_value=1, max_value=50, value=10)
        
        col_start, col_end, col_view = st.columns([1, 1, 1])
        with col_start:
            start_date_yt = st.date_input("ì˜ìƒ ê²Œì‹œ ì‹œì‘ì¼", datetime(2024, 1, 1))
        with col_end:
            end_date_yt = st.date_input("ì˜ìƒ ê²Œì‹œ ì¢…ë£Œì¼", datetime.now())
        with col_view:
            min_view_count = st.number_input("ìµœì†Œ ì¡°íšŒìˆ˜ ì»·", min_value=0, value=10000, step=1000)

        if st.button("í‚¤ì›Œë“œ ê²€ìƒ‰ ë° ìˆ˜ì§‘ ì‹œì‘", key="btn_yt_keyword"):
            if not yt_api_key:
                st.error("ë§¨ ìœ„ì— YouTube API Keyë¥¼ ë¨¼ì € ì…ë ¥í•´ì£¼ì„¸ìš”.")
            else:
                status_box = st.status("ë°ì´í„° ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...", expanded=True)
                youtube_data = []
                
                try:
                    youtube = build('youtube', 'v3', developerKey=yt_api_key)
                    start_dt = datetime.combine(start_date_yt, dt_time.min).isoformat() + "Z"
                    end_dt = datetime.combine(end_date_yt, dt_time.max).isoformat() + "Z"
                    
                    # 1. ì˜ìƒ ê²€ìƒ‰
                    search_response = youtube.search().list(
                        q=search_keyword, type='video', part='id', order='viewCount',
                        publishedAfter=start_dt, publishedBefore=end_dt, maxResults=max_videos
                    ).execute()
                    
                    video_ids = [item['id']['videoId'] for item in search_response.get('items', [])]
                    
                    if not video_ids:
                        status_box.update(label="ê²€ìƒ‰ëœ ì˜ìƒì´ ì—†ìŠµë‹ˆë‹¤.", state="error")
                    else:
                        # 2. ì¡°íšŒìˆ˜ í•„í„°ë§
                        stats_response = youtube.videos().list(
                            part='snippet,statistics', id=','.join(video_ids)
                        ).execute()
                        
                        target_videos = []
                        for v_item in stats_response.get('items', []):
                            views = int(v_item['statistics'].get('viewCount', 0))
                            if views >= min_view_count:
                                target_videos.append(v_item)
                        
                        if not target_videos:
                            status_box.update(label="ì¡°íšŒìˆ˜ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ì˜ìƒì´ ì—†ìŠµë‹ˆë‹¤.", state="error")
                        else:
                            # 3. ëŒ“ê¸€ ìˆ˜ì§‘
                            prog_bar = st.progress(0)
                            for idx, video in enumerate(target_videos):
                                vid = video['id']
                                v_title = video['snippet']['title']
                                v_channel = video['snippet']['channelTitle']
                                v_date = video['snippet']['publishedAt'][:10]
                                v_views = video['statistics'].get('viewCount', 0)
                                
                                status_box.write(f"Collecting: {v_title[:30]}...")
                                
                                try:
                                    # ëŒ“ê¸€ ê°€ì ¸ì˜¤ê¸° (ìµœëŒ€ 50ê°œ)
                                    comment_request = youtube.commentThreads().list(
                                        part="snippet", videoId=vid, maxResults=50, textFormat="plainText", order="relevance"
                                    )
                                    comment_response = comment_request.execute()
                                    
                                    for item in comment_response.get('items', []):
                                        c_snip = item['snippet']['topLevelComment']['snippet']
                                        youtube_data.append({
                                            'ì˜ìƒì œëª©': v_title, 'ì¡°íšŒìˆ˜': v_views, 'ì±„ë„ëª…': v_channel, 'ì˜ìƒê²Œì‹œì¼': v_date,
                                            'ì‘ì„±ì': c_snip['authorDisplayName'], 'ëŒ“ê¸€ë‚´ìš©': c_snip['textDisplay'],
                                            'ì¢‹ì•„ìš”': c_snip['likeCount'], 'ëŒ“ê¸€ì‘ì„±ì¼': c_snip['publishedAt'][:10]
                                        })
                                except: pass
                                prog_bar.progress((idx + 1) / len(target_videos))
                            
                            status_box.update(label="ì™„ë£Œ!", state="complete")
                            
                            if youtube_data:
                                df_yt = pd.DataFrame(youtube_data)
                                st.dataframe(df_yt)
                                st.download_button("ê²°ê³¼ ë‹¤ìš´ë¡œë“œ", df_yt.to_csv(index=False).encode('utf-8-sig'), f"yt_keyword_{search_keyword}.csv")
                            else: st.warning("ëŒ“ê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                except Exception as e:
                    status_box.update(label="ì—ëŸ¬ ë°œìƒ", state="error")
                    st.error(f"ì˜¤ë¥˜: {e}")

    # [TAB 2] ì‹ ê·œ ê¸°ëŠ¥: ê°œë³„ ì˜ìƒ ë§í¬
    with tab_yt2:
        st.caption("íŠ¹ì • YouTube ì˜ìƒì˜ ì£¼ì†Œ(URL)ë¥¼ ì…ë ¥í•˜ë©´, í•´ë‹¹ ì˜ìƒì˜ ëŒ“ê¸€ì„ ì§‘ì¤‘ì ìœ¼ë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")
        
        target_url = st.text_input("YouTube ì˜ìƒ ì£¼ì†Œ (URL)", placeholder="ì˜ˆ: https://www.youtube.com/watch?v=dQw4w9WgXcQ")
        max_comments_single = st.number_input("ìˆ˜ì§‘í•  ëŒ“ê¸€ ìˆ˜ (ìµœëŒ€)", min_value=10, max_value=500, value=100, step=10)

        if st.button("ë‹¨ì¼ ì˜ìƒ ëŒ“ê¸€ ìˆ˜ì§‘", key="btn_yt_link"):
            if not yt_api_key:
                st.error("ë§¨ ìœ„ì— YouTube API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            elif not target_url:
                st.error("ì˜ìƒ ì£¼ì†Œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            else:
                # URLì—ì„œ Video ID ì¶”ì¶œ ë¡œì§
                video_id = None
                if "v=" in target_url:
                    video_id = target_url.split("v=")[1].split("&")[0]
                elif "youtu.be" in target_url:
                    video_id = target_url.split("/")[-1].split("?")[0]
                
                if not video_id:
                    st.error("ì˜¬ë°”ë¥¸ YouTube URLì´ ì•„ë‹™ë‹ˆë‹¤.")
                else:
                    status_box = st.status(f"ì˜ìƒ ID: {video_id} ë¶„ì„ ì¤‘...", expanded=True)
                    single_yt_data = []
                    
                    try:
                        youtube = build('youtube', 'v3', developerKey=yt_api_key)
                        
                        # 1. ì˜ìƒ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                        video_response = youtube.videos().list(
                            part='snippet,statistics', id=video_id
                        ).execute()
                        
                        if not video_response.get('items'):
                            status_box.update(label="ì˜ìƒì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", state="error")
                        else:
                            v_info = video_response['items'][0]
                            v_title = v_info['snippet']['title']
                            v_channel = v_info['snippet']['channelTitle']
                            v_views = v_info['statistics'].get('viewCount', 0)
                            v_date = v_info['snippet']['publishedAt'][:10]
                            
                            status_box.write(f"ğŸ“º ì˜ìƒ ë°œê²¬: {v_title}")
                            status_box.write(f"ğŸ‘€ ì¡°íšŒìˆ˜: {v_views} | ğŸ“… ê²Œì‹œì¼: {v_date}")
                            
                            # 2. ëŒ“ê¸€ ìˆ˜ì§‘ (Paging ì²˜ë¦¬ë¡œ ë§ì´ ê°€ì ¸ì˜¤ê¸°)
                            comments_collected = 0
                            next_page_token = None
                            
                            while comments_collected < max_comments_single:
                                request = youtube.commentThreads().list(
                                    part="snippet", videoId=video_id, maxResults=100, 
                                    textFormat="plainText", pageToken=next_page_token, order="relevance"
                                )
                                response = request.execute()
                                
                                for item in response.get('items', []):
                                    c_snip = item['snippet']['topLevelComment']['snippet']
                                    single_yt_data.append({
                                        'ì˜ìƒì œëª©': v_title, 'ì‘ì„±ì': c_snip['authorDisplayName'],
                                        'ëŒ“ê¸€ë‚´ìš©': c_snip['textDisplay'], 'ì¢‹ì•„ìš”': c_snip['likeCount'],
                                        'ì‘ì„±ì¼': c_snip['publishedAt'][:10]
                                    })
                                    comments_collected += 1
                                    
                                next_page_token = response.get('nextPageToken')
                                if not next_page_token or comments_collected >= max_comments_single:
                                    break
                            
                            status_box.update(label="ìˆ˜ì§‘ ì™„ë£Œ!", state="complete")
                            
                            if single_yt_data:
                                df_single = pd.DataFrame(single_yt_data)
                                st.success(f"ì´ {len(df_single)}ê°œì˜ ëŒ“ê¸€ì„ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")
                                st.dataframe(df_single)
                                st.download_button("ê²°ê³¼ ë‹¤ìš´ë¡œë“œ", df_single.to_csv(index=False).encode('utf-8-sig'), f"yt_single_{video_id}.csv")
                            else:
                                st.warning("ëŒ“ê¸€ì´ ì—†ê±°ë‚˜ ëŒ“ê¸€ì´ ì¤‘ì§€ëœ ì˜ìƒì…ë‹ˆë‹¤.")
                                
                    except Exception as e:
                        status_box.update(label="ì—ëŸ¬ ë°œìƒ", state="error")
                        st.error(f"ì˜¤ë¥˜ ë‚´ìš©: {e}")

# =========================================================
# [SECTION 3] Reddit (ì¤€ë¹„ì¤‘)
# =========================================================
elif menu == "Reddit (ì¤€ë¹„ì¤‘)":
    st.info("Reddit í¬ë¡¤ëŸ¬ëŠ” ì¶”í›„ ì—…ë°ì´íŠ¸ ì˜ˆì •ì…ë‹ˆë‹¤.")