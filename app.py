import streamlit as st
import requests
import pandas as pd
import time
import random
import urllib3
from datetime import datetime, time as dt_time
from bs4 import BeautifulSoup
from googleapiclient.discovery import build

# --- ğŸ“Š ì‹œê°í™” ë¼ì´ë¸ŒëŸ¬ë¦¬ ---
from kiwipiepy import Kiwi
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from collections import Counter
import matplotlib.font_manager as fm

# SSL ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# í˜ì´ì§€ ê¸°ë³¸ ì„¤ì •
st.set_page_config(page_title="Steam & YouTube ë°ì´í„° ìˆ˜ì§‘ê¸°", layout="wide")

# --- ğŸ“Š ì‹œê°í™” ì—”ì§„ (ì–¸ì–´ë³„ ë¶„ì„ ê¸°ëŠ¥ íƒ‘ì¬) ---
def visualize_data(df, col_name):
    """
    [Final] ì–¸ì–´ë³„ ë…ë¦½ ë¶„ì„ ì‹œê°í™” ì—”ì§„
    ì‚¬ìš©ìê°€ í•œêµ­ì–´/ì˜ì–´ë¥¼ ì„ íƒí•˜ë©´ í•´ë‹¹ ì–¸ì–´ì˜ í‚¤ì›Œë“œë§Œ ë¶„ì„í•˜ì—¬ ë³´ì—¬ì¤ë‹ˆë‹¤.
    """
    if df is None or df.empty:
        return

    st.divider()
    st.subheader(f"ğŸ“Š {len(df)}ê°œ ë°ì´í„° í‚¤ì›Œë“œ ë¶„ì„")
    
    # 1. ğŸ›ï¸ ì–¸ì–´ ì„ íƒ ë“œë¡­ë‹¤ìš´ (í•œêµ­ì–´ vs ì˜ì–´)
    lang_option = st.selectbox(
        "ë¶„ì„í•  ì–¸ì–´ë¥¼ ì„ íƒí•˜ì„¸ìš”:",
        ["ğŸ‡°ğŸ‡· í•œêµ­ì–´", "ğŸ‡ºğŸ‡¸ ì˜ì–´"],
        index=0
    )
    
    with st.spinner(f"ğŸ’¬ {lang_option} ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ê³  ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
        try:
            kiwi = Kiwi()
            
            # 2. ë¶ˆìš©ì–´(Stopwords) ì •ì˜ - ì–¸ì–´ë³„ ë¶„ë¦¬
            stop_words_kr = {
                'ê²Œì„', 'ì§„ì§œ', 'ë„ˆë¬´', 'ì•„ë‹ˆ', 'ê·¼ë°', 'ì†”ì§íˆ', 'ê·¸ëƒ¥', 'ì´ê±°', 'ì •ë§', 
                'ìƒê°', 'ì‚¬ëŒ', 'í•˜ê³ ', 'í•´ì„œ', 'ìˆëŠ”', 'ì—†ëŠ”', 'ì…ë‹ˆë‹¤', 'í•©ë‹ˆë‹¤', 'ê·¸ê²Œ', 'ì¡´ë‚˜', 'ë•Œë¬¸ì—',
                'ìŠ¤íŒ€', 'í”Œë ˆì´', 'ì •ë„', 'í•˜ë‚˜', 'ì§€ê¸ˆ', 'ì¼ë‹¨', 'ë­”ê°€', 'ë³´ê³ ', 'í•˜ë©´', 'í•´ì„œ', 'í•˜ê²Œ', 'ê°™ì•„ìš”', 'ì¢‹ì•„ìš”'
            }
            
            stop_words_en = {
                'the', 'a', 'an', 'is', 'are', 'was', 'were', 'to', 'of', 'in', 'for', 'on', 'with', 'at', 'by', 'it', 'this', 'that',
                'and', 'but', 'or', 'so', 'if', 'not', 'no', 'yes', 'can', 'will', 'my', 'your', 'he', 'she', 'they', 'we',
                'game', 'games', 'play', 'playing', 'player', 'played', 'review', 'steam', 'fun', 'good', 'bad', 'best', 'like', 'just', 'more',
                'time', 'story', 'really', 'very', 'much', 'get', 'even', 'make', 'made', 'about', 'from', 'out'
            }
            
            # 3. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
            text_list = df[col_name].dropna().astype(str).tolist()
            full_text = " ".join(text_list)
            
            # ì†ë„ ìµœì í™” (ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ëŠ” ìë¦„)
            if len(full_text) > 100000:
                full_text = full_text[:100000]
                st.caption("â€» ë°ì´í„°ê°€ ë„ˆë¬´ ë§ì•„ ë¶„ì„ ì†ë„ë¥¼ ìœ„í•´ ì¼ë¶€ í…ìŠ¤íŠ¸ë§Œ ìƒ˜í”Œë§í–ˆìŠµë‹ˆë‹¤.")

            # 4. í† í°í™” ë° í‚¤ì›Œë“œ ì¶”ì¶œ
            tokens = kiwi.tokenize(full_text)
            keywords = []

            # [í•µì‹¬] ì„ íƒëœ ì–¸ì–´ì— ë”°ë¼ ë¡œì§ ë¶„ë¦¬
            if lang_option == "ğŸ‡°ğŸ‡· í•œêµ­ì–´":
                for t in tokens:
                    # í•œêµ­ì–´ ëª…ì‚¬(NNG, NNP)ë§Œ ì¶”ì¶œ
                    if t.tag in ['NNG', 'NNP'] and len(t.form) > 1:
                        if t.form not in stop_words_kr:
                            keywords.append(t.form)
                            
            elif lang_option == "ğŸ‡ºğŸ‡¸ ì˜ì–´":
                for t in tokens:
                    # ì˜ì–´ ì•ŒíŒŒë²³(SL)ë§Œ ì¶”ì¶œ
                    if t.tag == 'SL' and len(t.form) > 2:
                        word_lower = t.form.lower()
                        if word_lower not in stop_words_en:
                            keywords.append(word_lower)
            
            if not keywords:
                st.warning(f"ì„ íƒí•˜ì‹  ì–¸ì–´({lang_option})ë¡œ ì‘ì„±ëœ ìœ ì˜ë¯¸í•œ ë‹¨ì–´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return

            # 5. ë¹ˆë„ìˆ˜ ê³„ì‚°
            count = Counter(keywords)
            top_20 = dict(count.most_common(20))

        except Exception as e:
            st.error(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            return

    # --- ì‹œê°í™” í™”ë©´ êµ¬ì„± ---
    col_vis1, col_vis2 = st.columns(2)

    with col_vis1:
        st.markdown(f"#### â˜ï¸ ì›Œë“œ í´ë¼ìš°ë“œ ({lang_option})")
        try:
            # í°íŠ¸ ì„¤ì • (GitHubì— ì˜¬ë¦° í°íŠ¸ íŒŒì¼ëª…ê³¼ ì¼ì¹˜í•´ì•¼ í•¨)
            font_path = "NanumGothic.ttf" 
            try:
                wc = WordCloud(
                    font_path=font_path, 
                    background_color='white',
                    width=600,
                    height=400,
                    max_words=100
                ).generate_from_frequencies(count)
            except:
                # í°íŠ¸ íŒŒì¼ ì—†ì„ ì‹œ ê¸°ë³¸ í°íŠ¸
                wc = WordCloud(
                    background_color='white',
                    width=600,
                    height=400,
                    max_words=100
                ).generate_from_frequencies(count)

            fig = plt.figure(figsize=(10, 6))
            plt.imshow(wc, interpolation='bilinear')
            plt.axis('off')
            st.pyplot(fig)
            if lang_option == "ğŸ‡°ğŸ‡· í•œêµ­ì–´":
                st.caption("â€» í•œê¸€ì´ â–¡â–¡ë¡œ ë³´ì¸ë‹¤ë©´ `NanumGothic.ttf` íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        except Exception as e:
            st.error(f"ì›Œë“œ í´ë¼ìš°ë“œ ìƒì„± ì‹¤íŒ¨: {e}")

    with col_vis2:
        st.markdown(f"#### ğŸ“Š í•µì‹¬ í‚¤ì›Œë“œ Top 10 ({lang_option})")
        top_10 = dict(list(top_20.items())[:10])
        st.bar_chart(top_10, color="#FF4B4B")
        
        with st.expander("ğŸ“‹ ìƒì„¸ ë°ì´í„° ë³´ê¸°"):
            st.dataframe(pd.DataFrame(list(top_20.items()), columns=['í‚¤ì›Œë“œ', 'ë¹ˆë„ìˆ˜']), use_container_width=True)


# --- ğŸ” ë¹„ë°€ë²ˆí˜¸ ì ê¸ˆ ---
password = st.text_input("ì ‘ì† ì•”í˜¸", type="password")
if password != "smilegate":
    st.warning("ì•”í˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    st.stop()

st.title("Steam & YouTube ë°ì´í„° ìˆ˜ì§‘ê¸° (Visualized)")

# --- ì‚¬ì´ë“œë°” ---
with st.sidebar:
    st.header("ì„¤ì •")
    menu = st.selectbox("ë¶„ì„ ì±„ë„", ["Steam (ìŠ¤íŒ€)", "YouTube (ìœ íŠœë¸Œ)", "4chan (í•´ì™¸ í¬ëŸ¼)", "ë””ì‹œì¸ì‚¬ì´ë“œ"])
    st.divider()
    st.info("ğŸ’¡ **ì‹œê°í™” ê¸°ëŠ¥ ì•ˆë‚´**\n\n'Steam ë¦¬ë·°'ì™€ 'YouTube ëŒ“ê¸€' ìˆ˜ì§‘ ì‹œì—ë§Œ í•˜ë‹¨ì— ì›Œë“œ í´ë¼ìš°ë“œì™€ ë¶„ì„ ì°¨íŠ¸ê°€ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.")

# =========================================================
# [SECTION 1] Steam (ìŠ¤íŒ€)
# =========================================================
if menu == "Steam (ìŠ¤íŒ€)":
    tab1, tab2 = st.tabs(["ë¦¬ë·° ìˆ˜ì§‘ (API) - ğŸ“Šì‹œê°í™”", "í† ë¡ ì¥ ìˆ˜ì§‘ (í¬ë¡¤ë§)"])
    
    # [TAB 1] ë¦¬ë·° ìˆ˜ì§‘ (ì‹œê°í™” ì ìš© O)
    with tab1:
        st.subheader("ë¦¬ë·° ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„")
        col1, col2 = st.columns(2)
        with col1:
            app_id_review = st.text_input("App ID (ë¦¬ë·°ìš©)", value="1562700")
        with col2:
            language = st.selectbox("ì–¸ì–´", ["all", "koreana", "english", "japanese", "schinese"], index=0)
        
        col_start, col_end = st.columns(2)
        with col_start:
            start_date = st.date_input("ìˆ˜ì§‘ ì‹œì‘ ë‚ ì§œ", datetime(2024, 1, 1))
        with col_end:
            end_date = st.date_input("ìˆ˜ì§‘ ì¢…ë£Œ ë‚ ì§œ", datetime.now())
        
        if st.button("ë¦¬ë·° ìˆ˜ì§‘ ì‹œì‘", key="btn_review"):
            all_reviews = []
            cursor = '*'
            status_box = st.info(f"ë°ì´í„° ìˆ˜ì§‘ ì¤‘... (ëª©í‘œ ê¸°ê°„: {start_date} ~ {end_date})")
            
            try:
                for i in range(200): 
                    params = {
                        'json': 1, 'cursor': cursor, 'language': language,
                        'num_per_page': 100, 'purchase_type': 'all', 'filter': 'recent'
                    }
                    res = requests.get(f"https://store.steampowered.com/appreviews/{app_id_review}", params=params, verify=False)
                    data = res.json()
                    
                    if 'reviews' in data and len(data['reviews']) > 0:
                        last_ts = data['reviews'][-1]['timestamp_created']
                        curr_date = pd.to_datetime(last_ts, unit='s').date()
                        
                        for r in data['reviews']:
                            r_date = pd.to_datetime(r['timestamp_created'], unit='s').date()
                            if r_date > end_date: continue
                            if r_date < start_date: pass 
                            
                            if start_date <= r_date <= end_date:
                                all_reviews.append({
                                    'ì‘ì„±ì¼': r_date, 
                                    'ë‚´ìš©': r['review'].replace('\n', ' '), 
                                    'ì¶”ì²œìˆ˜': r['votes_up'],
                                    'í”Œë ˆì´ì‹œê°„(ë¶„)': r['author'].get('playtime_forever', 0)
                                })
                        
                        cursor = data['cursor']
                        status_box.info(f"í˜„ì¬ {len(all_reviews)}ê°œ ìˆ˜ì§‘ë¨... (í˜„ì¬ íƒìƒ‰ ìœ„ì¹˜: {curr_date})")
                        
                        if curr_date < start_date: break
                    else: break
                
                if all_reviews:
                    df = pd.DataFrame(all_reviews)
                    df = df.sort_values(by='ì‘ì„±ì¼', ascending=False)
                    status_box.success(f"ì™„ë£Œ! {start_date} ~ {end_date} ê¸°ê°„ì˜ ë¦¬ë·° {len(df)}ê°œë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
                    
                    st.dataframe(df)
                    st.download_button("ì—‘ì…€ ë‹¤ìš´ë¡œë“œ", df.to_csv(index=False).encode('utf-8-sig'), "steam_reviews.csv")

                    # ğŸ”¥ [ì‹œê°í™” ì—”ì§„ ê°€ë™]
                    visualize_data(df, "ë‚´ìš©")

                else:
                    st.warning("í•´ë‹¹ ê¸°ê°„ì— ì‘ì„±ëœ ë¦¬ë·°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            except Exception as e:
                st.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")

    # [TAB 2] í† ë¡ ì¥ ìˆ˜ì§‘ (ì‹œê°í™” ì ìš© X)
    with tab2:
        st.subheader("í† ë¡ ì¥ ìƒì„¸ ìˆ˜ì§‘ (ë³¸ë¬¸+ëŒ“ê¸€)")
        st.caption("â€» í† ë¡ ì¥ì€ í…ìŠ¤íŠ¸ êµ¬ì¡°ê°€ ë³µì¡í•˜ì—¬ í˜„ì¬ ì‹œê°í™” ê¸°ëŠ¥ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        target_url = st.text_input("ìˆ˜ì§‘í•  í† ë¡ ì¥ URL", value="https://steamcommunity.com/app/1562700/discussions/")
        pages_to_crawl = st.number_input("íƒìƒ‰í•  í˜ì´ì§€ ìˆ˜", min_value=1, max_value=20, value=2)
        
        if st.button("í† ë¡ ê¸€ ìˆ˜ì§‘ ì‹œì‘", key="btn_discuss"):
            discussion_data = []
            progress_bar = st.progress(0)
            status_text = st.empty()
            headers = {'User-Agent': 'Mozilla/5.0', 'Accept-Language': 'ko-KR'}
            cookies = {'wants_mature_content': '1', 'birthtime': '660000001', 'lastagecheckage': '1-January-1990'}
            
            try:
                if not target_url.endswith('/') and '?' not in target_url: target_url += '/'
                for p in range(pages_to_crawl):
                    full_url = f"{target_url}?fp={p+1}"
                    status_text.text(f"{p+1}í˜ì´ì§€ ìˆ˜ì§‘ ì¤‘...")
                    time.sleep(1)
                    res = requests.get(full_url, headers=headers, cookies=cookies, verify=False)
                    soup = BeautifulSoup(res.text, 'html.parser')
                    topic_rows = soup.find_all('div', class_='forum_topic')
                    
                    if not topic_rows: break
                    
                    for idx, row in enumerate(topic_rows):
                        try:
                            link_tag = row.find('a', class_='forum_topic_overlay')
                            title_tag = row.find('div', class_='forum_topic_name')
                            if not link_tag: continue
                            link = link_tag['href']
                            title = title_tag.text.strip() if title_tag else "ì œëª© ì—†ìŒ"
                            
                            time.sleep(0.5)
                            sub_res = requests.get(link, headers=headers, cookies=cookies, verify=False)
                            sub_soup = BeautifulSoup(sub_res.text, 'html.parser')
                            
                            op_div = sub_soup.find('div', class_='forum_op')
                            if op_div:
                                author = op_div.find('a', class_='forum_op_author').text.strip()
                                content = op_div.find('div', class_='content').text.strip()
                                discussion_data.append({'êµ¬ë¶„': 'ê²Œì‹œê¸€', 'ì œëª©': title, 'ì‘ì„±ì': author, 'ë‚´ìš©': content, 'ë§í¬': link})
                            
                            comments = sub_soup.find_all('div', class_='commentthread_comment')
                            for comm in comments:
                                c_text = comm.find('div', class_='commentthread_comment_text').text.strip()
                                c_author = comm.find('a', class_='commentthread_author_link').text.strip()
                                if c_text:
                                    discussion_data.append({'êµ¬ë¶„': 'ëŒ“ê¸€', 'ì œëª©': f"(Re) {title}", 'ì‘ì„±ì': c_author, 'ë‚´ìš©': c_text, 'ë§í¬': link})
                        except: continue
                        progress_bar.progress(min((p / pages_to_crawl) + ((idx + 1) / len(topic_rows) / pages_to_crawl), 0.99))
                
                progress_bar.progress(1.0)
                if discussion_data:
                    df = pd.DataFrame(discussion_data)
                    st.success(f"ìˆ˜ì§‘ ì™„ë£Œ! ì´ {len(df)}ê°œ")
                    st.dataframe(df)
                    st.download_button("ì—‘ì…€ ë‹¤ìš´ë¡œë“œ", df.to_csv(index=False).encode('utf-8-sig'), "steam_discussion.csv")
                else: st.error("ë°ì´í„° ì—†ìŒ")
            except Exception as e: st.error(f"ì˜¤ë¥˜: {e}")

# =========================================================
# [SECTION 2] YouTube (ìœ íŠœë¸Œ)
# =========================================================
elif menu == "YouTube (ìœ íŠœë¸Œ)":
    st.subheader("ğŸŸ¥ YouTube ë°ì´í„° ìˆ˜ì§‘ê¸°")
    yt_api_key = st.text_input("YouTube Data API Key", type="password")

    tab_yt1, tab_yt2 = st.tabs(["ğŸ” í‚¤ì›Œë“œ ê²€ìƒ‰ - ğŸ“Šì‹œê°í™”", "ğŸ”— ê°œë³„ ì˜ìƒ ë§í¬ - ğŸ“Šì‹œê°í™”"])

    # [TAB 1] í‚¤ì›Œë“œ ê²€ìƒ‰ (ì‹œê°í™” ì ìš© O)
    with tab_yt1:
        st.caption("íŠ¹ì • í‚¤ì›Œë“œ(ê²Œì„ëª… ë“±)ë¥¼ ê²€ìƒ‰í•˜ì—¬ ëŒ“ê¸€ì„ ìˆ˜ì§‘í•˜ê³  ë¶„ì„í•©ë‹ˆë‹¤.")
        col1, col2 = st.columns([3, 1])
        with col1:
            search_keyword = st.text_input("ê²€ìƒ‰ì–´ (ì˜ˆ: Elden Ring Review)", value="Elden Ring")
        with col2:
            max_videos = st.number_input("ë¶„ì„í•  ì˜ìƒ ìˆ˜", min_value=1, max_value=50, value=10)
        
        col_start, col_end, col_view = st.columns([1, 1, 1])
        with col_start:
            start_date_yt = st.date_input("ì˜ìƒ ê²Œì‹œ ì‹œì‘ì¼", datetime(2024, 1, 1))
        with col_end:
            end_date_yt = st.date_input("ì˜ìƒ ê²Œì‹œ ì¢…ë£Œì¼", datetime.now())
        with col_view:
            min_view_count = st.number_input("ìµœì†Œ ì¡°íšŒìˆ˜ ì»·", min_value=0, value=10000, step=1000)

        if st.button("í‚¤ì›Œë“œ ê²€ìƒ‰ ë° ìˆ˜ì§‘ ì‹œì‘", key="btn_yt_keyword"):
            if not yt_api_key:
                st.error("ë§¨ ìœ„ì— YouTube API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            else:
                status_box = st.status("ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„ ì¤‘...", expanded=True)
                youtube_data = []
                
                try:
                    youtube = build('youtube', 'v3', developerKey=yt_api_key)
                    start_dt = datetime.combine(start_date_yt, dt_time.min).isoformat() + "Z"
                    end_dt = datetime.combine(end_date_yt, dt_time.max).isoformat() + "Z"
                    
                    search_response = youtube.search().list(
                        q=search_keyword, type='video', part='id', order='viewCount',
                        publishedAfter=start_dt, publishedBefore=end_dt, maxResults=max_videos
                    ).execute()
                    
                    video_ids = [item['id']['videoId'] for item in search_response.get('items', [])]
                    
                    if not video_ids:
                        status_box.update(label="ê²€ìƒ‰ëœ ì˜ìƒì´ ì—†ìŠµë‹ˆë‹¤.", state="error")
                    else:
                        stats_response = youtube.videos().list(
                            part='snippet,statistics', id=','.join(video_ids)
                        ).execute()
                        
                        target_videos = []
                        for v_item in stats_response.get('items', []):
                            views = int(v_item['statistics'].get('viewCount', 0))
                            if views >= min_view_count:
                                target_videos.append(v_item)
                        
                        if not target_videos:
                            status_box.update(label="ì¡°íšŒìˆ˜ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ì˜ìƒì´ ì—†ìŠµë‹ˆë‹¤.", state="error")
                        else:
                            prog_bar = st.progress(0)
                            for idx, video in enumerate(target_videos):
                                vid = video['id']
                                v_title = video['snippet']['title']
                                v_views = video['statistics'].get('viewCount', 0)
                                v_date = video['snippet']['publishedAt'][:10]
                                
                                status_box.write(f"Collecting comments from: {v_title[:30]}...")
                                
                                try:
                                    comment_request = youtube.commentThreads().list(
                                        part="snippet", videoId=vid, maxResults=50, textFormat="plainText", order="relevance"
                                    )
                                    comment_response = comment_request.execute()
                                    
                                    for item in comment_response.get('items', []):
                                        c_snip = item['snippet']['topLevelComment']['snippet']
                                        youtube_data.append({
                                            'ì˜ìƒì œëª©': v_title, 'ì¡°íšŒìˆ˜': v_views, 'ì˜ìƒê²Œì‹œì¼': v_date,
                                            'ì‘ì„±ì': c_snip['authorDisplayName'], 'ëŒ“ê¸€ë‚´ìš©': c_snip['textDisplay'],
                                            'ì¢‹ì•„ìš”': c_snip['likeCount'], 'ëŒ“ê¸€ì‘ì„±ì¼': c_snip['publishedAt'][:10]
                                        })
                                except: pass
                                prog_bar.progress((idx + 1) / len(target_videos))
                            
                            status_box.update(label="ìˆ˜ì§‘ ì™„ë£Œ!", state="complete")
                            
                            if youtube_data:
                                df_yt = pd.DataFrame(youtube_data)
                                st.dataframe(df_yt)
                                st.download_button("ê²°ê³¼ ë‹¤ìš´ë¡œë“œ", df_yt.to_csv(index=False).encode('utf-8-sig'), f"yt_keyword_{search_keyword}.csv")
                                
                                # ğŸ”¥ [ì‹œê°í™” ì—”ì§„ ê°€ë™]
                                visualize_data(df_yt, "ëŒ“ê¸€ë‚´ìš©")
                            else: st.warning("ëŒ“ê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                except Exception as e:
                    status_box.update(label="ì—ëŸ¬ ë°œìƒ", state="error")
                    st.error(f"ì˜¤ë¥˜: {e}")

    # [TAB 2] ê°œë³„ ì˜ìƒ ë§í¬ (ì‹œê°í™” ì ìš© O)
    with tab_yt2:
        st.caption("ê°œë³„ ì˜ìƒì˜ ëŒ“ê¸€ì„ ì§‘ì¤‘ì ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.")
        target_url = st.text_input("YouTube ì˜ìƒ ì£¼ì†Œ (URL)", placeholder="ì˜ˆ: https://www.youtube.com/watch?v=...")
        max_comments_single = st.number_input("ìˆ˜ì§‘í•  ëŒ“ê¸€ ìˆ˜ (ìµœëŒ€)", min_value=10, max_value=500, value=100, step=10)

        if st.button("ë‹¨ì¼ ì˜ìƒ ëŒ“ê¸€ ìˆ˜ì§‘", key="btn_yt_link"):
            if not yt_api_key or not target_url:
                st.error("API Keyì™€ ì˜ìƒ ì£¼ì†Œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
            else:
                video_id = None
                if "v=" in target_url: video_id = target_url.split("v=")[1].split("&")[0]
                elif "youtu.be" in target_url: video_id = target_url.split("/")[-1].split("?")[0]
                
                if not video_id:
                    st.error("ì˜¬ë°”ë¥¸ YouTube URLì´ ì•„ë‹™ë‹ˆë‹¤.")
                else:
                    status_box = st.status(f"ì˜ìƒ ID: {video_id} ë¶„ì„ ì¤‘...", expanded=True)
                    single_yt_data = []
                    try:
                        youtube = build('youtube', 'v3', developerKey=yt_api_key)
                        
                        # ì˜ìƒ ì •ë³´ í™•ì¸
                        video_response = youtube.videos().list(part='snippet,statistics', id=video_id).execute()
                        if not video_response.get('items'):
                            status_box.update(label="ì˜ìƒì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", state="error")
                        else:
                            v_info = video_response['items'][0]
                            v_title = v_info['snippet']['title']
                            status_box.write(f"ğŸ“º ë¶„ì„ ëŒ€ìƒ: {v_title}")

                            # ëŒ“ê¸€ ìˆ˜ì§‘
                            comments_collected = 0
                            next_page_token = None
                            
                            while comments_collected < max_comments_single:
                                request = youtube.commentThreads().list(
                                    part="snippet", videoId=video_id, maxResults=100, 
                                    textFormat="plainText", pageToken=next_page_token, order="relevance"
                                )
                                response = request.execute()
                                
                                for item in response.get('items', []):
                                    c_snip = item['snippet']['topLevelComment']['snippet']
                                    single_yt_data.append({
                                        'ì˜ìƒì œëª©': v_title, 'ì‘ì„±ì': c_snip['authorDisplayName'],
                                        'ëŒ“ê¸€ë‚´ìš©': c_snip['textDisplay'], 'ì¢‹ì•„ìš”': c_snip['likeCount'],
                                        'ì‘ì„±ì¼': c_snip['publishedAt'][:10]
                                    })
                                    comments_collected += 1
                                
                                next_page_token = response.get('nextPageToken')
                                if not next_page_token or comments_collected >= max_comments_single: break
                            
                            status_box.update(label="ìˆ˜ì§‘ ì™„ë£Œ!", state="complete")
                            
                            if single_yt_data:
                                df_single = pd.DataFrame(single_yt_data)
                                st.success(f"ì´ {len(df_single)}ê°œì˜ ëŒ“ê¸€ì„ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
                                st.dataframe(df_single)
                                st.download_button("ê²°ê³¼ ë‹¤ìš´ë¡œë“œ", df_single.to_csv(index=False).encode('utf-8-sig'), f"yt_single_{video_id}.csv")
                                
                                # ğŸ”¥ [ì‹œê°í™” ì—”ì§„ ê°€ë™]
                                visualize_data(df_single, "ëŒ“ê¸€ë‚´ìš©")
                            else:
                                st.warning("ëŒ“ê¸€ì´ ì—†ê±°ë‚˜ ì°¨ë‹¨ëœ ì˜ìƒì…ë‹ˆë‹¤.")
                    except Exception as e:
                        status_box.update(label="ì—ëŸ¬ ë°œìƒ", state="error")
                        st.error(f"ì˜¤ë¥˜: {e}")

# =========================================================
# [SECTION 3] 4chan (í¬ì±ˆ) - ì‹œê°í™” ì œì™¸
# =========================================================
elif menu == "4chan (í•´ì™¸ í¬ëŸ¼)": 
    st.subheader("ğŸ€ 4chan (/v/ - Video Games) ì‹¤ì‹œê°„ ë°˜ì‘")
    col1, col2 = st.columns([3, 1])
    with col1:
        search_keyword = st.text_input("ê²€ìƒ‰ì–´ (ì˜ì–´, ì˜ˆ: Elden Ring)", value="Elden Ring")
    with col2:
        result_limit = st.number_input("ê°€ì ¸ì˜¬ ìŠ¤ë ˆë“œ ìˆ˜", min_value=1, max_value=20, value=3)

    if st.button("4chan ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘", key="btn_4chan"):
        status_box = st.status("4chan ìŠ¤ìº” ì¤‘...", expanded=True)
        fourchan_data = []
        try:
            catalog_url = "https://a.4cdn.org/v/catalog.json"
            res = requests.get(catalog_url, verify=False)
            if res.status_code == 200:
                pages = res.json()
                found_threads = []
                for page in pages:
                    for thread in page.get('threads', []):
                        title = thread.get('sub', '') 
                        comment = thread.get('com', '')
                        if search_keyword.lower() in title.lower() or search_keyword.lower() in comment.lower():
                            found_threads.append(thread['no'])
                            if len(found_threads) >= result_limit: break
                    if len(found_threads) >= result_limit: break
                
                if found_threads:
                    status_box.write(f"âœ… {len(found_threads)}ê°œ ìŠ¤ë ˆë“œ ë°œê²¬. ìƒì„¸ ìˆ˜ì§‘ ì¤‘...")
                    progress_bar = st.progress(0)
                    for idx, thread_id in enumerate(found_threads):
                        thread_url = f"https://a.4cdn.org/v/thread/{thread_id}.json"
                        t_res = requests.get(thread_url, verify=False)
                        if t_res.status_code == 200:
                            posts = t_res.json().get('posts', [])
                            op_post = posts[0]
                            op_content = BeautifulSoup(op_post.get('com', ''), "html.parser").get_text()
                            fourchan_data.append({
                                'êµ¬ë¶„': 'ì›ê¸€', 'ë‚´ìš©': op_content, 'ì‘ì„±ì¼': datetime.fromtimestamp(op_post['time']).strftime('%Y-%m-%d %H:%M')
                            })
                            for reply in posts[1:]:
                                reply_content = BeautifulSoup(reply.get('com', ''), "html.parser").get_text()
                                fourchan_data.append({
                                    'êµ¬ë¶„': 'ëŒ“ê¸€', 'ë‚´ìš©': reply_content, 'ì‘ì„±ì¼': datetime.fromtimestamp(reply['time']).strftime('%Y-%m-%d %H:%M')
                                })
                        time.sleep(0.5)
                        progress_bar.progress((idx + 1) / len(found_threads))
                    
                    status_box.update(label="ì™„ë£Œ!", state="complete")
                    if fourchan_data:
                        df_4chan = pd.DataFrame(fourchan_data)
                        st.dataframe(df_4chan)
                        st.download_button("ì—‘ì…€ ë‹¤ìš´ë¡œë“œ", df_4chan.to_csv(index=False).encode('utf-8-sig'), f"4chan_{search_keyword}.csv")
                else: status_box.update(label="ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ", state="error")
            else: st.error("ì ‘ì† ì‹¤íŒ¨")
        except Exception as e: st.error(f"ì˜¤ë¥˜: {e}")

# =========================================================
# [SECTION 4] ë””ì‹œì¸ì‚¬ì´ë“œ - ì‹œê°í™” ì œì™¸
# =========================================================
elif menu == "ë””ì‹œì¸ì‚¬ì´ë“œ":
    st.subheader("ğŸ”µ ë””ì‹œì¸ì‚¬ì´ë“œ ê°¤ëŸ¬ë¦¬ ìˆ˜ì§‘")
    col1, col2 = st.columns(2)
    with col1:
        gallery_id = st.text_input("ê°¤ëŸ¬ë¦¬ ID", value="indiegame")
        is_minor = st.checkbox("ë§ˆì´ë„ˆ ê°¤ëŸ¬ë¦¬ ì—¬ë¶€", value=True)
    with col2:
        keyword = st.text_input("ê²€ìƒ‰ì–´", value="")
        pages_to_crawl = st.number_input("í˜ì´ì§€ ìˆ˜", value=1)

    if st.button("ë””ì‹œì¸ì‚¬ì´ë“œ ìˆ˜ì§‘ ì‹œì‘", key="btn_dc"):
        status_box = st.status("ì ‘ì† ì¤‘...", expanded=True)
        dc_data = []
        base_url = "https://gall.dcinside.com/mgallery/board/lists/" if is_minor else "https://gall.dcinside.com/board/lists/"
        target_referer = f"{base_url}?id={gallery_id}"
        
        # ëª¨ë°”ì¼ ìœ„ì¥ í—¤ë” ì‚¬ìš© (ì°¨ë‹¨ ìš°íšŒìš©)
        headers = {
            'User-Agent': 'Mozilla/5.0 (Linux; Android 10; SM-G981B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.162 Mobile Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Referer': target_referer,
            'Connection': 'keep-alive'
        }

        try:
            progress_bar = st.progress(0)
            for i in range(pages_to_crawl):
                params = {'id': gallery_id, 'page': i+1}
                if keyword:
                    params['s_type'] = 'search_subject_memo'
                    params['s_keyword'] = keyword
                
                # ëœë¤ ë”œë ˆì´ (ë´‡ íƒì§€ ìš°íšŒ)
                wait_time = random.uniform(2, 4)
                status_box.write(f"â³ {i+1}í˜ì´ì§€ ìˆ˜ì§‘ ì „ {wait_time:.1f}ì´ˆ ëŒ€ê¸°...")
                time.sleep(wait_time)
                
                res = requests.get(base_url, headers=headers, params=params)
                if res.status_code == 200:
                    soup = BeautifulSoup(res.text, 'html.parser')
                    rows = soup.find_all('tr', class_='ub-content')
                    for row in rows:
                        try:
                            if 'ub-notice' in row.get('class', []): continue
                            title_tag = row.find('td', class_='gall_tit').find('a')
                            title = title_tag.text.strip()
                            dc_data.append({'ê°¤ëŸ¬ë¦¬ID': gallery_id, 'ì œëª©': title})
                        except: continue
                    progress_bar.progress((i + 1) / pages_to_crawl)
                else:
                    st.error(f"ì ‘ì† ì‹¤íŒ¨ Code: {res.status_code}")
                    break
            
            status_box.update(label="ì™„ë£Œ!", state="complete")
            if dc_data:
                df_dc = pd.DataFrame(dc_data)
                st.dataframe(df_dc)
                st.download_button("ì—‘ì…€ ë‹¤ìš´ë¡œë“œ", df_dc.to_csv(index=False).encode('utf-8-sig'), f"dc_{gallery_id}.csv")
            else: st.warning("ë°ì´í„° ì—†ìŒ")
        except Exception as e: st.error(f"ì˜¤ë¥˜: {e}")