import streamlit as st
import requests
import pandas as pd
import time
import random
import urllib3
from datetime import datetime, time as dt_time
from bs4 import BeautifulSoup
from googleapiclient.discovery import build

# SSL ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# í˜ì´ì§€ ê¸°ë³¸ ì„¤ì •
st.set_page_config(page_title="Steam & YouTube ë°ì´í„° ìˆ˜ì§‘ê¸°", layout="wide")

# --- ğŸ” ë¹„ë°€ë²ˆí˜¸ ì ê¸ˆ (êµ¬ì¡° ìœ ì§€) ---
password = st.text_input("ì ‘ì† ì•”í˜¸", type="password")
if password != "smilegate":
    st.warning("ì•”í˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    st.stop()

st.title("Steam & YouTube ë°ì´í„° ìˆ˜ì§‘ê¸°")

# --- ì‚¬ì´ë“œë°” (êµ¬ì¡° ìœ ì§€) ---
with st.sidebar:
    st.header("ì„¤ì •")
    menu = st.selectbox("ë¶„ì„ ì±„ë„", ["Steam (ìŠ¤íŒ€)", "YouTube (ìœ íŠœë¸Œ)", "Reddit (ì¤€ë¹„ì¤‘)"])
    st.divider()

# =========================================================
# [SECTION 1] Steam (ìŠ¤íŒ€)
# =========================================================
if menu == "Steam (ìŠ¤íŒ€)":
    tab1, tab2 = st.tabs(["ë¦¬ë·° ìˆ˜ì§‘ (API)", "í† ë¡ ì¥ ìˆ˜ì§‘ (í¬ë¡¤ë§)"])
    
    # [TAB 1] ë¦¬ë·° ìˆ˜ì§‘ (ê¸°ê°„ ì„¤ì • ê¸°ëŠ¥ ê°•í™”ë¨)
    with tab1:
        st.subheader("ë¦¬ë·° ë°ì´í„° ìˆ˜ì§‘")
        col1, col2 = st.columns(2)
        with col1:
            app_id_review = st.text_input("App ID (ë¦¬ë·°ìš©)", value="1562700")
        with col2:
            language = st.selectbox("ì–¸ì–´", ["all", "koreana", "english", "japanese", "schinese"], index=0)
        
        # --- [ë³€ê²½] ì‹œì‘ì¼/ì¢…ë£Œì¼ ê¸°ê°„ ì„¤ì • ì¶”ê°€ ---
        col_start, col_end = st.columns(2)
        with col_start:
            start_date = st.date_input("ìˆ˜ì§‘ ì‹œì‘ ë‚ ì§œ", datetime(2024, 1, 1))
        with col_end:
            end_date = st.date_input("ìˆ˜ì§‘ ì¢…ë£Œ ë‚ ì§œ", datetime.now())
        
        if st.button("ë¦¬ë·° ìˆ˜ì§‘ ì‹œì‘", key="btn_review"):
            all_reviews = []
            cursor = '*'
            status_box = st.info(f"ë°ì´í„° ìˆ˜ì§‘ ì¤‘... (ëª©í‘œ ê¸°ê°„: {start_date} ~ {end_date})")
            
            try:
                # ì•ˆì „ ì¥ì¹˜: ìµœëŒ€ 200í˜ì´ì§€ (ì•½ 2ë§Œê°œ)
                for i in range(200): 
                    params = {
                        'json': 1, 'cursor': cursor, 'language': language,
                        'num_per_page': 100, 'purchase_type': 'all', 'filter': 'recent' # ìµœì‹ ìˆœ ì •ë ¬
                    }
                    res = requests.get(f"https://store.steampowered.com/appreviews/{app_id_review}", params=params, verify=False)
                    data = res.json()
                    
                    if 'reviews' in data and len(data['reviews']) > 0:
                        # í˜„ì¬ í˜ì´ì§€ì˜ ë§ˆì§€ë§‰ ë¦¬ë·° ë‚ ì§œ í™•ì¸
                        last_ts = data['reviews'][-1]['timestamp_created']
                        curr_date = pd.to_datetime(last_ts, unit='s').date()
                        
                        for r in data['reviews']:
                            r_date = pd.to_datetime(r['timestamp_created'], unit='s').date()
                            
                            # [ë¡œì§] ì¢…ë£Œ ë‚ ì§œë³´ë‹¤ ë” ìµœì‹  ê¸€ì´ë©´? -> ìˆ˜ì§‘ ì•ˆ í•˜ê³  íŒ¨ìŠ¤ (Continue)
                            if r_date > end_date:
                                continue
                            
                            # [ë¡œì§] ì‹œì‘ ë‚ ì§œë³´ë‹¤ ë” ê³¼ê±° ê¸€ì´ë©´? -> ìˆ˜ì§‘ ì¤‘ë‹¨ (Break)
                            if r_date < start_date:
                                # ì—¬ê¸°ì„œ ë°”ë¡œ breakí•˜ë©´ ì´ì¤‘ ë£¨í”„ íƒˆì¶œì´ ì•ˆ ë˜ë¯€ë¡œ í”Œë˜ê·¸ ì‚¬ìš©í•˜ê±°ë‚˜ ì˜ˆì™¸ ì²˜ë¦¬ í•„ìš”
                                # ê°„ë‹¨íˆ ì—¬ê¸°ì„  ë¹ˆ ë¦¬ìŠ¤íŠ¸ì— ë”ë¯¸ ì¶”ê°€í•´ì„œ ë°”ê¹¥ì—ì„œ ì²´í¬í•˜ë„ë¡ í•¨
                                pass 
                            
                            # ê¸°ê°„ ë‚´ì˜ ê¸€ì´ë©´ ìˆ˜ì§‘
                            if start_date <= r_date <= end_date:
                                all_reviews.append({
                                    'ì‘ì„±ì¼': r_date, 
                                    'ë‚´ìš©': r['review'].replace('\n', ' '), 
                                    'ì¶”ì²œìˆ˜': r['votes_up'],
                                    'í”Œë ˆì´ì‹œê°„(ë¶„)': r['author'].get('playtime_forever', 0)
                                })
                        
                        cursor = data['cursor']
                        status_box.info(f"í˜„ì¬ {len(all_reviews)}ê°œ ìˆ˜ì§‘ë¨... (í˜„ì¬ íƒìƒ‰ ìœ„ì¹˜: {curr_date})")
                        
                        # íƒìƒ‰ ìœ„ì¹˜ê°€ ì‹œì‘ ë‚ ì§œë³´ë‹¤ ê³¼ê±°ë¡œ ë„˜ì–´ê°€ë©´ ì „ì²´ ë°˜ë³µ ì¢…ë£Œ
                        if curr_date < start_date: 
                            break
                    else: 
                        break
                
                if all_reviews:
                    df = pd.DataFrame(all_reviews)
                    # í˜¹ì‹œ ëª¨ë¥¼ ì¤‘ë³µì´ë‚˜ ê²½ê³„ê°’ ì •ë ¬
                    df = df.sort_values(by='ì‘ì„±ì¼', ascending=False)
                    
                    st.success(f"ì™„ë£Œ! {start_date} ~ {end_date} ê¸°ê°„ì˜ ë¦¬ë·° {len(df)}ê°œë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
                    st.dataframe(df)
                    st.download_button("ì—‘ì…€ ë‹¤ìš´ë¡œë“œ", df.to_csv(index=False).encode('utf-8-sig'), "steam_reviews.csv")
                else:
                    st.warning("í•´ë‹¹ ê¸°ê°„ì— ì‘ì„±ëœ ë¦¬ë·°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            except Exception as e:
                st.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")

    # [TAB 2] í† ë¡ ì¥ ìˆ˜ì§‘ (êµ¬ì¡° ìœ ì§€)
    with tab2:
        st.subheader("í† ë¡ ì¥ ìƒì„¸ ìˆ˜ì§‘ (ë³¸ë¬¸+ëŒ“ê¸€)")
        st.caption("â€» í† ë¡ ì¥ì€ ì›¹ í¬ë¡¤ë§ ë°©ì‹ì´ë¼ 'í˜ì´ì§€ ìˆ˜'ë¡œë§Œ ë²”ìœ„ë¥¼ ì§€ì •í•©ë‹ˆë‹¤.")
        
        target_url = st.text_input("ìˆ˜ì§‘í•  í† ë¡ ì¥ URL", value="https://steamcommunity.com/app/1562700/discussions/")
        pages_to_crawl = st.number_input("íƒìƒ‰í•  í˜ì´ì§€ ìˆ˜", min_value=1, max_value=20, value=2)
        
        if st.button("í† ë¡ ê¸€ ìˆ˜ì§‘ ì‹œì‘", key="btn_discuss"):
            discussion_data = []
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7'
            }
            cookies = {'wants_mature_content': '1', 'birthtime': '660000001', 'lastagecheckage': '1-January-1990'}
            
            try:
                if not target_url.endswith('/') and '?' not in target_url: target_url += '/'

                for p in range(pages_to_crawl):
                    full_url = f"{target_url}?fp={p+1}"
                    status_text.text(f"{p+1}í˜ì´ì§€ ëª©ë¡ì„ ì½ê³  ìˆìŠµë‹ˆë‹¤...")
                    time.sleep(random.uniform(1.0, 2.0))
                    res = requests.get(full_url, headers=headers, cookies=cookies, verify=False, timeout=15)
                    soup = BeautifulSoup(res.text, 'html.parser')
                    topic_rows = soup.find_all('div', class_='forum_topic')
                    
                    if not topic_rows:
                        st.warning(f"{p+1}í˜ì´ì§€ì—ì„œ ê¸€ ëª©ë¡ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                        break
                    
                    status_text.text(f"{p+1}í˜ì´ì§€: {len(topic_rows)}ê°œ ê¸€ ë°œê²¬. ìƒì„¸ ë‚´ìš©ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤...")
                    
                    for idx, row in enumerate(topic_rows):
                        try:
                            link_tag = row.find('a', class_='forum_topic_overlay')
                            title_tag = row.find('div', class_='forum_topic_name')
                            if not link_tag: continue
                            link = link_tag['href']
                            title = title_tag.text.strip() if title_tag else "ì œëª© ì—†ìŒ"
                            
                            time.sleep(random.uniform(0.5, 1.0))
                            sub_res = requests.get(link, headers=headers, cookies=cookies, verify=False)
                            sub_soup = BeautifulSoup(sub_res.text, 'html.parser')
                            
                            op_div = sub_soup.find('div', class_='forum_op')
                            if op_div:
                                author = op_div.find('a', class_='forum_op_author').text.strip()
                                content = op_div.find('div', class_='content').text.strip()
                                discussion_data.append({'êµ¬ë¶„': 'ê²Œì‹œê¸€', 'ì œëª©': title, 'ì‘ì„±ì': author, 'ë‚´ìš©': content, 'ë§í¬': link})
                            
                            comments = sub_soup.find_all('div', class_='commentthread_comment')
                            for comm in comments:
                                c_text = comm.find('div', class_='commentthread_comment_text').text.strip()
                                c_author = comm.find('a', class_='commentthread_author_link').text.strip()
                                if c_text:
                                    discussion_data.append({'êµ¬ë¶„': 'ëŒ“ê¸€', 'ì œëª©': f"(Re) {title}", 'ì‘ì„±ì': c_author, 'ë‚´ìš©': c_text, 'ë§í¬': link})
                        except: continue
                        progress_bar.progress(min((p / pages_to_crawl) + ((idx + 1) / len(topic_rows) / pages_to_crawl), 0.99))

                progress_bar.progress(1.0)
                if discussion_data:
                    df = pd.DataFrame(discussion_data)
                    st.success(f"ìˆ˜ì§‘ ì™„ë£Œ! ì´ {len(df)}ê°œì˜ ë°ì´í„°")
                    st.dataframe(df)
                    st.download_button("ì—‘ì…€ ë‹¤ìš´ë¡œë“œ", df.to_csv(index=False).encode('utf-8-sig'), "steam_discussion_final.csv")
                else: st.error("ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            except Exception as e: st.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")

# =========================================================
# [SECTION 2] YouTube (ìœ íŠœë¸Œ) - ê¸°ì¡´ ì½”ë“œ ìœ ì§€
# =========================================================
elif menu == "YouTube (ìœ íŠœë¸Œ)":
    st.subheader("ğŸŸ¥ YouTube ì˜ìƒ ë° ëŒ“ê¸€ ìˆ˜ì§‘")
    
    col1, col2 = st.columns([3, 1])
    with col1:
        yt_api_key = st.text_input("YouTube Data API Key", type="password")
    with col2:
        max_videos = st.number_input("ê²€ìƒ‰í•  ì˜ìƒ ìˆ˜", min_value=1, max_value=50, value=10)

    search_keyword = st.text_input("ê²€ìƒ‰ì–´ (ê²Œì„ëª…)", value="Elden Ring")
    
    col_start, col_end, col_view = st.columns([1, 1, 1])
    with col_start:
        start_date_yt = st.date_input("ì‹œì‘ ë‚ ì§œ", datetime(2024, 1, 1))
    with col_end:
        end_date_yt = st.date_input("ì¢…ë£Œ ë‚ ì§œ", datetime.now())
    with col_view:
        min_view_count = st.number_input("ìµœì†Œ ì¡°íšŒìˆ˜ ì œí•œ", min_value=0, value=10000, step=1000)

    st.markdown("---")

    if st.button("YouTube ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘", key="btn_yt"):
        if not yt_api_key:
            st.error("YouTube API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        else:
            status_box = st.status("ë°ì´í„° ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...", expanded=True)
            youtube_data = []
            
            try:
                youtube = build('youtube', 'v3', developerKey=yt_api_key)
                
                start_dt = datetime.combine(start_date_yt, dt_time.min).isoformat() + "Z"
                end_dt = datetime.combine(end_date_yt, dt_time.max).isoformat() + "Z"
                
                status_box.write("ğŸ” ì¡°ê±´ì— ë§ëŠ” ì˜ìƒì„ ê²€ìƒ‰ ì¤‘ì…ë‹ˆë‹¤...")
                search_response = youtube.search().list(
                    q=search_keyword,
                    type='video',
                    part='id',
                    order='viewCount',
                    publishedAfter=start_dt,
                    publishedBefore=end_dt,
                    maxResults=max_videos
                ).execute()
                
                video_ids = [item['id']['videoId'] for item in search_response.get('items', [])]
                
                if not video_ids:
                    status_box.update(label="ê²€ìƒ‰ëœ ì˜ìƒì´ ì—†ìŠµë‹ˆë‹¤.", state="error")
                else:
                    status_box.write(f"ğŸ“Š ê²€ìƒ‰ëœ {len(video_ids)}ê°œ ì˜ìƒì˜ ì¡°íšŒìˆ˜ë¥¼ í™•ì¸ ì¤‘...")
                    
                    stats_response = youtube.videos().list(
                        part='snippet,statistics',
                        id=','.join(video_ids)
                    ).execute()
                    
                    target_videos = []
                    for v_item in stats_response.get('items', []):
                        views = int(v_item['statistics'].get('viewCount', 0))
                        if views >= min_view_count:
                            target_videos.append(v_item)
                    
                    if not target_videos:
                        status_box.update(label=f"ì¡°íšŒìˆ˜ {min_view_count}íšŒ ì´ìƒì¸ ì˜ìƒì´ ì—†ìŠµë‹ˆë‹¤.", state="error")
                    else:
                        status_box.write(f"âœ… ì¡°íšŒìˆ˜ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” {len(target_videos)}ê°œ ì˜ìƒì—ì„œ ëŒ“ê¸€ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")
                        
                        prog_bar = st.progress(0)
                        
                        for idx, video in enumerate(target_videos):
                            vid = video['id']
                            v_title = video['snippet']['title']
                            v_channel = video['snippet']['channelTitle']
                            v_date = video['snippet']['publishedAt'][:10]
                            v_views = video['statistics'].get('viewCount', 0)
                            
                            status_box.write(f"Collecting: {v_title[:30]} (ì¡°íšŒìˆ˜: {v_views})...")
                            
                            try:
                                comment_request = youtube.commentThreads().list(
                                    part="snippet",
                                    videoId=vid,
                                    maxResults=50,
                                    textFormat="plainText",
                                    order="relevance"
                                )
                                comment_response = comment_request.execute()
                                
                                for item in comment_response.get('items', []):
                                    c_snip = item['snippet']['topLevelComment']['snippet']
                                    youtube_data.append({
                                        'ì˜ìƒì œëª©': v_title,
                                        'ì¡°íšŒìˆ˜': v_views,
                                        'ì±„ë„ëª…': v_channel,
                                        'ì˜ìƒê²Œì‹œì¼': v_date,
                                        'ì‘ì„±ì': c_snip['authorDisplayName'],
                                        'ëŒ“ê¸€ë‚´ìš©': c_snip['textDisplay'],
                                        'ì¢‹ì•„ìš”': c_snip['likeCount'],
                                        'ëŒ“ê¸€ì‘ì„±ì¼': c_snip['publishedAt'][:10]
                                    })
                            except: pass
                            
                            prog_bar.progress((idx + 1) / len(target_videos))
                            time.sleep(0.1)
                        
                        status_box.update(label="ì‘ì—… ì™„ë£Œ!", state="complete")
                        
                        if youtube_data:
                            df_yt = pd.DataFrame(youtube_data)
                            st.success(f"ì´ {len(df_yt)}ê°œì˜ ëŒ“ê¸€ì„ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
                            st.dataframe(df_yt)
                            st.download_button("YouTube ê²°ê³¼ ë‹¤ìš´ë¡œë“œ", df_yt.to_csv(index=False).encode('utf-8-sig'), f"youtube_{search_keyword}.csv", "text/csv")
                        else:
                            st.warning("ìˆ˜ì§‘ ê°€ëŠ¥í•œ ëŒ“ê¸€ì´ ì—†ìŠµë‹ˆë‹¤.")

            except Exception as e:
                status_box.update(label="ì—ëŸ¬ ë°œìƒ", state="error")
                st.error(f"ì˜¤ë¥˜ ë‚´ìš©: {e}")

# =========================================================
# [SECTION 3] Reddit (ì¤€ë¹„ì¤‘)
# =========================================================
elif menu == "Reddit (ì¤€ë¹„ì¤‘)":
    st.info("Reddit í¬ë¡¤ëŸ¬ëŠ” ì¶”í›„ ì—…ë°ì´íŠ¸ ì˜ˆì •ì…ë‹ˆë‹¤.")