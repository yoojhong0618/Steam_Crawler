import streamlit as st
import requests
import pandas as pd
import time
from datetime import datetime
from bs4 import BeautifulSoup

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ìŠ¤íŒ€ ë¦¬ë·° & í† ë¡  ìˆ˜ì§‘ê¸°", layout="wide")

# --- ğŸ” ë¹„ë°€ë²ˆí˜¸ ì ê¸ˆ ---
password = st.text_input("ğŸ”’ ì ‘ì† ì•”í˜¸", type="password")
if password != "smilegate":
    st.warning("ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤.")
    st.stop()

st.title("Steam ë¦¬ë·° & í† ë¡  ìˆ˜ì§‘ê¸°")

# --- ì‚¬ì´ë“œë°” ---
with st.sidebar:
    st.header("ì„¤ì •")
    menu = st.selectbox("ë¶„ì„ ì±„ë„", ["Steam (ìŠ¤íŒ€)", "Reddit (ì¤€ë¹„ì¤‘)", "YouTube (ì¤€ë¹„ì¤‘)"])
    st.divider()

if menu == "Steam (ìŠ¤íŒ€)":
    tab1, tab2 = st.tabs(["â­ ë¦¬ë·° ìˆ˜ì§‘", "ğŸ—£ï¸ í† ë¡ ì¥ ìˆ˜ì§‘"])
    
    # [TAB 1] ë¦¬ë·° ìˆ˜ì§‘ (ê¸°ì¡´ê³¼ ë™ì¼)
    with tab1:
        col1, col2 = st.columns(2)
        with col1:
            app_id_review = st.text_input("App ID (ë¦¬ë·°ìš©)", value="1562700")
        with col2:
            language = st.selectbox("ì–¸ì–´", ["all", "koreana", "english", "japanese", "schinese"], index=0)
        start_date = st.date_input("ì‹œì‘ì¼", datetime(2025, 2, 1))
        
        if st.button("ë¦¬ë·° ìˆ˜ì§‘ ì‹œì‘", key="btn_review"):
            # (ê¸°ì¡´ ë¦¬ë·° ì½”ë“œ ìƒëµ - ì˜ ë˜ë‹ˆê¹Œ ê·¸ëŒ€ë¡œ ë‘ì…”ë„ ë©ë‹ˆë‹¤)
            st.toast("ê¸°ì¡´ ë¦¬ë·° ìˆ˜ì§‘ ë¡œì§ ì‹¤í–‰")
            # ... (ì´ì „ ì½”ë“œ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì‹œë©´ ë©ë‹ˆë‹¤) ...

    # [TAB 2] í† ë¡ ì¥ ìˆ˜ì§‘ (ëª©ë¡ -> ìƒì„¸ ë‚´ìš© -> ëŒ“ê¸€ê¹Œì§€ ìˆ˜ì§‘)
    with tab2:
        st.info("ğŸ’¡ 1. ëª©ë¡ í˜ì´ì§€(ì´ë¯¸ì§€1)ë¥¼ ì½ê³  -> 2. ê° ê¸€(ì´ë¯¸ì§€2)ë¡œ ë“¤ì–´ê°€ì„œ ë‚´ìš©ê³¼ ëŒ“ê¸€ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.")
        
        # URL ì§ì ‘ ì…ë ¥ ë°©ì‹ (ì§ˆë¬¸ìë‹˜ì´ ì›í•˜ì‹œëŠ” ê·¸ ì£¼ì†Œ!)
        target_url = st.text_input(
            "ìˆ˜ì§‘í•  í† ë¡ ì¥ URL", 
            value="https://steamcommunity.com/app/1562700/discussions/"
        )
        
        pages_to_crawl = st.number_input("íƒìƒ‰í•  í˜ì´ì§€ ìˆ˜ (ëª©ë¡ í˜ì´ì§€ ê¸°ì¤€)", min_value=1, max_value=50, value=3)
        
        if st.button("í† ë¡ ê¸€ ìƒì„¸ ìˆ˜ì§‘ ì‹œì‘", key="btn_discuss"):
            st.toast("ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
            discussion_data = []
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7'
            }
            cookies = {'wants_mature_content': '1', 'birthtime': '660000001', 'lastagecheckage': '1-January-1990'}
            
            try:
                # URL ëì— '/'ê°€ ì—†ìœ¼ë©´ ë¶™ì—¬ì¤Œ (ì•ˆì „ì¥ì¹˜)
                if not target_url.endswith('/') and '?' not in target_url:
                    target_url += '/'

                for p in range(pages_to_crawl):
                    # 1. ëª©ë¡ í˜ì´ì§€ ì ‘ì† (ì´ë¯¸ì§€ 1)
                    full_url = f"{target_url}?fp={p+1}"
                    
                    res = requests.get(full_url, headers=headers, cookies=cookies) 
                    soup = BeautifulSoup(res.text, 'html.parser')
                    
                    # ê¸€ ëª©ë¡(ì œëª©+ë§í¬) ì°¾ê¸°
                    topics = soup.find_all('a', class_='forum_topic_link')
                    
                    # [ì˜ˆì™¸ì²˜ë¦¬] ëª©ë¡ì´ ì—†ìœ¼ë©´? (ë¡œë¹„ í˜ì´ì§€ê±°ë‚˜ ì—ëŸ¬)
                    if len(topics) == 0:
                        st.warning(f"âš ï¸ {p+1}í˜ì´ì§€ì—ì„œ ê¸€ ëª©ë¡ì„ ëª» ì°¾ì•˜ìŠµë‹ˆë‹¤.")
                        
                        # í˜¹ì‹œ 'ì¼ë°˜ í† ë¡ ' ë§í¬ê°€ ìˆëŠ”ì§€ ì°¾ì•„ë´…ë‹ˆë‹¤ (ìë™ ê¸¸ì°¾ê¸°)
                        general_link = soup.find('a', class_='forum_link', string=lambda t: "General" in t if t else False)
                        if general_link:
                            st.info(f"ğŸ‘‰ '{general_link.text.strip()}' ê²Œì‹œíŒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. URLì„ {general_link['href']} ë¡œ ë°”ê¿”ì„œ ë‹¤ì‹œ ì‹œë„í•´ë³´ì„¸ìš”.")
                        else:
                            with st.expander("ê°œë°œììš© íŒíŠ¸ (ì ‘ì† í™”ë©´)"):
                                st.write(f"ì ‘ì† URL: {full_url}")
                                st.write("í˜ì´ì§€ ì œëª©: " + (soup.title.string.strip() if soup.title else "ì—†ìŒ"))
                        break 
                    
                    status_text.text(f"ğŸ“„ {p+1}í˜ì´ì§€ ëª©ë¡ í™•ë³´! ({len(topics)}ê°œ ê¸€). ìƒì„¸ ë‚´ìš©ì„ ê¸ì–´ì˜µë‹ˆë‹¤...")
                    
                    # 2. ê° ê¸€ ì•ˆìœ¼ë¡œ ë“¤ì–´ê°€ê¸° (ì´ë¯¸ì§€ 2 - Deep Dive)
                    for idx, topic in enumerate(topics):
                        title = topic.text.strip()
                        link = topic['href']
                        
                        # ìƒì„¸ í˜ì´ì§€ ì ‘ì†
                        sub_res = requests.get(link, headers=headers, cookies=cookies)
                        sub_soup = BeautifulSoup(sub_res.text, 'html.parser')
                        
                        # (A) ë³¸ë¬¸ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
                        content_div = sub_soup.find('div', class_='forum_op')
                        if content_div:
                            author = content_div.find('div', class_='author').text.strip()
                            main_text = content_div.find('div', class_='content').text.strip()
                            date_posted = content_div.find('div', class_='date').text.strip()
                            
                            # ê²Œì‹œê¸€ ë°ì´í„° ì €ì¥
                            post_item = {
                                'Type': 'ê²Œì‹œê¸€(ë³¸ë¬¸)', 
                                'Title': title, 
                                'Author': author, 
                                'Content': main_text, 
                                'Date': date_posted, 
                                'Link': link
                            }
                            discussion_data.append(post_item)
                            
                            # (B) ëŒ“ê¸€ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
                            comments = sub_soup.find_all('div', class_='commentthread_comment')
                            for comm in comments:
                                try:
                                    c_author = comm.find('bdi').text.strip()
                                    c_text = comm.find('div', class_='commentthread_comment_text').text.strip()
                                    
                                    # ëŒ“ê¸€ ë°ì´í„° ì €ì¥ (ì œëª©ì€ ë³¸ë¬¸ ì œëª© ë”°ë¼ê°)
                                    comment_item = {
                                        'Type': 'ã„´ëŒ“ê¸€', 
                                        'Title': f"(Re) {title}", 
                                        'Author': c_author, 
                                        'Content': c_text, 
                                        'Date': '-', 
                                        'Link': link
                                    }
                                    discussion_data.append(comment_item)
                                except: continue
                        
                        # ë„ˆë¬´ ë¹ ë¥´ë©´ ì°¨ë‹¨ë‹¹í•˜ë‹ˆê¹Œ ì‚´ì§ ì‰¬ê¸°
                        time.sleep(0.5)
                        
                        # ì§„í–‰ìƒí™© ì—…ë°ì´íŠ¸ (ëª©ë¡ 1ê°œ ì²˜ë¦¬í•  ë•Œë§ˆë‹¤)
                        current_progress = (p / pages_to_crawl) + ((idx + 1) / len(topics) / pages_to_crawl)
                        progress_bar.progress(min(current_progress, 0.99))

                progress_bar.progress(1.0)
                
                if discussion_data:
                    df = pd.DataFrame(discussion_data)
                    st.success(f"ğŸ‰ ìˆ˜ì§‘ ì™„ë£Œ! ì´ {len(df)}ê°œì˜ ê¸€ê³¼ ëŒ“ê¸€ì„ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")
                    st.dataframe(df)
                    st.download_button("í† ë¡ ì¥ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ", df.to_csv(index=False).encode('utf-8-sig'), "steam_discuss_full.csv")
                else:
                    st.error("ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                    
            except Exception as e:
                st.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")