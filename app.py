import streamlit as st
import requests
import pandas as pd
import time
import random
import urllib3
from datetime import datetime, time as dt_time
from bs4 import BeautifulSoup
from googleapiclient.discovery import build

# SSL ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# í˜ì´ì§€ ê¸°ë³¸ ì„¤ì •
st.set_page_config(page_title="Steam & YouTube ë°ì´í„° ìˆ˜ì§‘ê¸°", layout="wide")

# --- ğŸ” ë¹„ë°€ë²ˆí˜¸ ì ê¸ˆ ---
password = st.text_input("ì ‘ì† ì•”í˜¸", type="password")
if password != "smilegate":
    st.warning("ì•”í˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    st.stop()

st.title("Steam & YouTube ë°ì´í„° ìˆ˜ì§‘ê¸°")

# --- ì‚¬ì´ë“œë°” ---
with st.sidebar:
    st.header("ì„¤ì •")
    menu = st.selectbox("ë¶„ì„ ì±„ë„", ["Steam (ìŠ¤íŒ€)", "YouTube (ìœ íŠœë¸Œ)", "4chan (í•´ì™¸ í¬ëŸ¼)", "ë””ì‹œì¸ì‚¬ì´ë“œ"])
    st.divider()

# =========================================================
# [SECTION 1] Steam (ìŠ¤íŒ€) - ê¸°ì¡´ ì½”ë“œ ìœ ì§€
# =========================================================
if menu == "Steam (ìŠ¤íŒ€)":
    tab1, tab2 = st.tabs(["ë¦¬ë·° ìˆ˜ì§‘ (API)", "í† ë¡ ì¥ ìˆ˜ì§‘ (í¬ë¡¤ë§)"])
    
    # [TAB 1] ë¦¬ë·° ìˆ˜ì§‘
    with tab1:
        st.subheader("ë¦¬ë·° ë°ì´í„° ìˆ˜ì§‘")
        col1, col2 = st.columns(2)
        with col1:
            app_id_review = st.text_input("App ID (ë¦¬ë·°ìš©)", value="1562700")
        with col2:
            language = st.selectbox("ì–¸ì–´", ["all", "koreana", "english", "japanese", "schinese"], index=0)
        
        col_start, col_end = st.columns(2)
        with col_start:
            start_date = st.date_input("ìˆ˜ì§‘ ì‹œì‘ ë‚ ì§œ", datetime(2024, 1, 1))
        with col_end:
            end_date = st.date_input("ìˆ˜ì§‘ ì¢…ë£Œ ë‚ ì§œ", datetime.now())
        
        if st.button("ë¦¬ë·° ìˆ˜ì§‘ ì‹œì‘", key="btn_review"):
            all_reviews = []
            cursor = '*'
            status_box = st.info(f"ë°ì´í„° ìˆ˜ì§‘ ì¤‘... (ëª©í‘œ ê¸°ê°„: {start_date} ~ {end_date})")
            
            try:
                for i in range(200): 
                    params = {
                        'json': 1, 'cursor': cursor, 'language': language,
                        'num_per_page': 100, 'purchase_type': 'all', 'filter': 'recent'
                    }
                    res = requests.get(f"https://store.steampowered.com/appreviews/{app_id_review}", params=params, verify=False)
                    data = res.json()
                    
                    if 'reviews' in data and len(data['reviews']) > 0:
                        last_ts = data['reviews'][-1]['timestamp_created']
                        curr_date = pd.to_datetime(last_ts, unit='s').date()
                        
                        for r in data['reviews']:
                            r_date = pd.to_datetime(r['timestamp_created'], unit='s').date()
                            if r_date > end_date: continue
                            if r_date < start_date: pass 
                            
                            if start_date <= r_date <= end_date:
                                all_reviews.append({
                                    'ì‘ì„±ì¼': r_date, 
                                    'ë‚´ìš©': r['review'].replace('\n', ' '), 
                                    'ì¶”ì²œìˆ˜': r['votes_up'],
                                    'í”Œë ˆì´ì‹œê°„(ë¶„)': r['author'].get('playtime_forever', 0)
                                })
                        
                        cursor = data['cursor']
                        status_box.info(f"í˜„ì¬ {len(all_reviews)}ê°œ ìˆ˜ì§‘ë¨... (í˜„ì¬ íƒìƒ‰ ìœ„ì¹˜: {curr_date})")
                        
                        if curr_date < start_date: break
                    else: break
                
                if all_reviews:
                    df = pd.DataFrame(all_reviews)
                    df = df.sort_values(by='ì‘ì„±ì¼', ascending=False)
                    st.success(f"ì™„ë£Œ! {start_date} ~ {end_date} ê¸°ê°„ì˜ ë¦¬ë·° {len(df)}ê°œë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
                    st.dataframe(df)
                    st.download_button("ì—‘ì…€ ë‹¤ìš´ë¡œë“œ", df.to_csv(index=False).encode('utf-8-sig'), "steam_reviews.csv")
                else:
                    st.warning("í•´ë‹¹ ê¸°ê°„ì— ì‘ì„±ëœ ë¦¬ë·°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            except Exception as e:
                st.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")

    # [TAB 2] í† ë¡ ì¥ ìˆ˜ì§‘
    with tab2:
        st.subheader("í† ë¡ ì¥ ìƒì„¸ ìˆ˜ì§‘ (ë³¸ë¬¸+ëŒ“ê¸€)")
        st.caption("â€» í† ë¡ ì¥ì€ ì›¹ í¬ë¡¤ë§ ë°©ì‹ì´ë¼ 'í˜ì´ì§€ ìˆ˜'ë¡œë§Œ ë²”ìœ„ë¥¼ ì§€ì •í•©ë‹ˆë‹¤.")
        target_url = st.text_input("ìˆ˜ì§‘í•  í† ë¡ ì¥ URL", value="https://steamcommunity.com/app/1562700/discussions/")
        pages_to_crawl = st.number_input("íƒìƒ‰í•  í˜ì´ì§€ ìˆ˜", min_value=1, max_value=20, value=2)
        
        if st.button("í† ë¡ ê¸€ ìˆ˜ì§‘ ì‹œì‘", key="btn_discuss"):
            discussion_data = []
            progress_bar = st.progress(0)
            status_text = st.empty()
            headers = {'User-Agent': 'Mozilla/5.0', 'Accept-Language': 'ko-KR'}
            cookies = {'wants_mature_content': '1', 'birthtime': '660000001', 'lastagecheckage': '1-January-1990'}
            
            try:
                if not target_url.endswith('/') and '?' not in target_url: target_url += '/'
                for p in range(pages_to_crawl):
                    full_url = f"{target_url}?fp={p+1}"
                    status_text.text(f"{p+1}í˜ì´ì§€ ìˆ˜ì§‘ ì¤‘...")
                    time.sleep(1)
                    res = requests.get(full_url, headers=headers, cookies=cookies, verify=False)
                    soup = BeautifulSoup(res.text, 'html.parser')
                    topic_rows = soup.find_all('div', class_='forum_topic')
                    
                    if not topic_rows: break
                    
                    for idx, row in enumerate(topic_rows):
                        try:
                            link_tag = row.find('a', class_='forum_topic_overlay')
                            title_tag = row.find('div', class_='forum_topic_name')
                            if not link_tag: continue
                            link = link_tag['href']
                            title = title_tag.text.strip() if title_tag else "ì œëª© ì—†ìŒ"
                            
                            time.sleep(0.5)
                            sub_res = requests.get(link, headers=headers, cookies=cookies, verify=False)
                            sub_soup = BeautifulSoup(sub_res.text, 'html.parser')
                            
                            op_div = sub_soup.find('div', class_='forum_op')
                            if op_div:
                                author = op_div.find('a', class_='forum_op_author').text.strip()
                                content = op_div.find('div', class_='content').text.strip()
                                discussion_data.append({'êµ¬ë¶„': 'ê²Œì‹œê¸€', 'ì œëª©': title, 'ì‘ì„±ì': author, 'ë‚´ìš©': content, 'ë§í¬': link})
                            
                            comments = sub_soup.find_all('div', class_='commentthread_comment')
                            for comm in comments:
                                c_text = comm.find('div', class_='commentthread_comment_text').text.strip()
                                c_author = comm.find('a', class_='commentthread_author_link').text.strip()
                                if c_text:
                                    discussion_data.append({'êµ¬ë¶„': 'ëŒ“ê¸€', 'ì œëª©': f"(Re) {title}", 'ì‘ì„±ì': c_author, 'ë‚´ìš©': c_text, 'ë§í¬': link})
                        except: continue
                        progress_bar.progress(min((p / pages_to_crawl) + ((idx + 1) / len(topic_rows) / pages_to_crawl), 0.99))
                
                progress_bar.progress(1.0)
                if discussion_data:
                    df = pd.DataFrame(discussion_data)
                    st.success(f"ìˆ˜ì§‘ ì™„ë£Œ! ì´ {len(df)}ê°œ")
                    st.dataframe(df)
                    st.download_button("ì—‘ì…€ ë‹¤ìš´ë¡œë“œ", df.to_csv(index=False).encode('utf-8-sig'), "steam_discussion.csv")
                else: st.error("ë°ì´í„° ì—†ìŒ")
            except Exception as e: st.error(f"ì˜¤ë¥˜: {e}")

# =========================================================
# [SECTION 2] YouTube (ìœ íŠœë¸Œ) - [êµ¬ì¡° ë³€ê²½ë¨]
# =========================================================
elif menu == "YouTube (ìœ íŠœë¸Œ)":
    st.subheader("ğŸŸ¥ YouTube ë°ì´í„° ìˆ˜ì§‘ê¸°")
    
    # API í‚¤ëŠ” ë‘ íƒ­ì—ì„œ ê³µí†µìœ¼ë¡œ ì“°ë¯€ë¡œ ë§¨ ìœ„ë¡œ ëºŒ
    yt_api_key = st.text_input("YouTube Data API Key", type="password")

    # íƒ­ ë¶„ë¦¬: í‚¤ì›Œë“œ ê²€ìƒ‰ vs ê°œë³„ ë§í¬
    tab_yt1, tab_yt2 = st.tabs(["ğŸ” í‚¤ì›Œë“œ ê²€ìƒ‰ (ë‹¤ìˆ˜ ì˜ìƒ)", "ğŸ”— ê°œë³„ ì˜ìƒ ë§í¬ (1ê°œ)"])

    # [TAB 1] ê¸°ì¡´ ê¸°ëŠ¥: í‚¤ì›Œë“œ ê²€ìƒ‰
    with tab_yt1:
        st.caption("íŠ¹ì • í‚¤ì›Œë“œ(ê²Œì„ëª… ë“±)ë¥¼ ê²€ìƒ‰í•˜ì—¬, ì¡°íšŒìˆ˜ê°€ ë†’ì€ ì˜ìƒë“¤ì˜ ëŒ“ê¸€ì„ í•œêº¼ë²ˆì— ìˆ˜ì§‘í•©ë‹ˆë‹¤.")
        
        col1, col2 = st.columns([3, 1])
        with col1:
            search_keyword = st.text_input("ê²€ìƒ‰ì–´ (ì˜ˆ: Elden Ring Review)", value="Elden Ring")
        with col2:
            max_videos = st.number_input("ë¶„ì„í•  ì˜ìƒ ìˆ˜", min_value=1, max_value=50, value=10)
        
        col_start, col_end, col_view = st.columns([1, 1, 1])
        with col_start:
            start_date_yt = st.date_input("ì˜ìƒ ê²Œì‹œ ì‹œì‘ì¼", datetime(2024, 1, 1))
        with col_end:
            end_date_yt = st.date_input("ì˜ìƒ ê²Œì‹œ ì¢…ë£Œì¼", datetime.now())
        with col_view:
            min_view_count = st.number_input("ìµœì†Œ ì¡°íšŒìˆ˜ ì»·", min_value=0, value=10000, step=1000)

        if st.button("í‚¤ì›Œë“œ ê²€ìƒ‰ ë° ìˆ˜ì§‘ ì‹œì‘", key="btn_yt_keyword"):
            if not yt_api_key:
                st.error("ë§¨ ìœ„ì— YouTube API Keyë¥¼ ë¨¼ì € ì…ë ¥í•´ì£¼ì„¸ìš”.")
            else:
                status_box = st.status("ë°ì´í„° ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...", expanded=True)
                youtube_data = []
                
                try:
                    youtube = build('youtube', 'v3', developerKey=yt_api_key)
                    start_dt = datetime.combine(start_date_yt, dt_time.min).isoformat() + "Z"
                    end_dt = datetime.combine(end_date_yt, dt_time.max).isoformat() + "Z"
                    
                    # 1. ì˜ìƒ ê²€ìƒ‰
                    search_response = youtube.search().list(
                        q=search_keyword, type='video', part='id', order='viewCount',
                        publishedAfter=start_dt, publishedBefore=end_dt, maxResults=max_videos
                    ).execute()
                    
                    video_ids = [item['id']['videoId'] for item in search_response.get('items', [])]
                    
                    if not video_ids:
                        status_box.update(label="ê²€ìƒ‰ëœ ì˜ìƒì´ ì—†ìŠµë‹ˆë‹¤.", state="error")
                    else:
                        # 2. ì¡°íšŒìˆ˜ í•„í„°ë§
                        stats_response = youtube.videos().list(
                            part='snippet,statistics', id=','.join(video_ids)
                        ).execute()
                        
                        target_videos = []
                        for v_item in stats_response.get('items', []):
                            views = int(v_item['statistics'].get('viewCount', 0))
                            if views >= min_view_count:
                                target_videos.append(v_item)
                        
                        if not target_videos:
                            status_box.update(label="ì¡°íšŒìˆ˜ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ì˜ìƒì´ ì—†ìŠµë‹ˆë‹¤.", state="error")
                        else:
                            # 3. ëŒ“ê¸€ ìˆ˜ì§‘
                            prog_bar = st.progress(0)
                            for idx, video in enumerate(target_videos):
                                vid = video['id']
                                v_title = video['snippet']['title']
                                v_channel = video['snippet']['channelTitle']
                                v_date = video['snippet']['publishedAt'][:10]
                                v_views = video['statistics'].get('viewCount', 0)
                                
                                status_box.write(f"Collecting: {v_title[:30]}...")
                                
                                try:
                                    # ëŒ“ê¸€ ê°€ì ¸ì˜¤ê¸° (ìµœëŒ€ 50ê°œ)
                                    comment_request = youtube.commentThreads().list(
                                        part="snippet", videoId=vid, maxResults=50, textFormat="plainText", order="relevance"
                                    )
                                    comment_response = comment_request.execute()
                                    
                                    for item in comment_response.get('items', []):
                                        c_snip = item['snippet']['topLevelComment']['snippet']
                                        youtube_data.append({
                                            'ì˜ìƒì œëª©': v_title, 'ì¡°íšŒìˆ˜': v_views, 'ì±„ë„ëª…': v_channel, 'ì˜ìƒê²Œì‹œì¼': v_date,
                                            'ì‘ì„±ì': c_snip['authorDisplayName'], 'ëŒ“ê¸€ë‚´ìš©': c_snip['textDisplay'],
                                            'ì¢‹ì•„ìš”': c_snip['likeCount'], 'ëŒ“ê¸€ì‘ì„±ì¼': c_snip['publishedAt'][:10]
                                        })
                                except: pass
                                prog_bar.progress((idx + 1) / len(target_videos))
                            
                            status_box.update(label="ì™„ë£Œ!", state="complete")
                            
                            if youtube_data:
                                df_yt = pd.DataFrame(youtube_data)
                                st.dataframe(df_yt)
                                st.download_button("ê²°ê³¼ ë‹¤ìš´ë¡œë“œ", df_yt.to_csv(index=False).encode('utf-8-sig'), f"yt_keyword_{search_keyword}.csv")
                            else: st.warning("ëŒ“ê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                except Exception as e:
                    status_box.update(label="ì—ëŸ¬ ë°œìƒ", state="error")
                    st.error(f"ì˜¤ë¥˜: {e}")

    # [TAB 2] ì‹ ê·œ ê¸°ëŠ¥: ê°œë³„ ì˜ìƒ ë§í¬
    with tab_yt2:
        st.caption("íŠ¹ì • YouTube ì˜ìƒì˜ ì£¼ì†Œ(URL)ë¥¼ ì…ë ¥í•˜ë©´, í•´ë‹¹ ì˜ìƒì˜ ëŒ“ê¸€ì„ ì§‘ì¤‘ì ìœ¼ë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")
        
        target_url = st.text_input("YouTube ì˜ìƒ ì£¼ì†Œ (URL)", placeholder="ì˜ˆ: https://www.youtube.com/watch?v=dQw4w9WgXcQ")
        max_comments_single = st.number_input("ìˆ˜ì§‘í•  ëŒ“ê¸€ ìˆ˜ (ìµœëŒ€)", min_value=10, max_value=500, value=100, step=10)

        if st.button("ë‹¨ì¼ ì˜ìƒ ëŒ“ê¸€ ìˆ˜ì§‘", key="btn_yt_link"):
            if not yt_api_key:
                st.error("ë§¨ ìœ„ì— YouTube API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            elif not target_url:
                st.error("ì˜ìƒ ì£¼ì†Œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            else:
                # URLì—ì„œ Video ID ì¶”ì¶œ ë¡œì§
                video_id = None
                if "v=" in target_url:
                    video_id = target_url.split("v=")[1].split("&")[0]
                elif "youtu.be" in target_url:
                    video_id = target_url.split("/")[-1].split("?")[0]
                
                if not video_id:
                    st.error("ì˜¬ë°”ë¥¸ YouTube URLì´ ì•„ë‹™ë‹ˆë‹¤.")
                else:
                    status_box = st.status(f"ì˜ìƒ ID: {video_id} ë¶„ì„ ì¤‘...", expanded=True)
                    single_yt_data = []
                    
                    try:
                        youtube = build('youtube', 'v3', developerKey=yt_api_key)
                        
                        # 1. ì˜ìƒ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                        video_response = youtube.videos().list(
                            part='snippet,statistics', id=video_id
                        ).execute()
                        
                        if not video_response.get('items'):
                            status_box.update(label="ì˜ìƒì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", state="error")
                        else:
                            v_info = video_response['items'][0]
                            v_title = v_info['snippet']['title']
                            v_channel = v_info['snippet']['channelTitle']
                            v_views = v_info['statistics'].get('viewCount', 0)
                            v_date = v_info['snippet']['publishedAt'][:10]
                            
                            status_box.write(f"ğŸ“º ì˜ìƒ ë°œê²¬: {v_title}")
                            status_box.write(f"ğŸ‘€ ì¡°íšŒìˆ˜: {v_views} | ğŸ“… ê²Œì‹œì¼: {v_date}")
                            
                            # 2. ëŒ“ê¸€ ìˆ˜ì§‘ (Paging ì²˜ë¦¬ë¡œ ë§ì´ ê°€ì ¸ì˜¤ê¸°)
                            comments_collected = 0
                            next_page_token = None
                            
                            while comments_collected < max_comments_single:
                                request = youtube.commentThreads().list(
                                    part="snippet", videoId=video_id, maxResults=100, 
                                    textFormat="plainText", pageToken=next_page_token, order="relevance"
                                )
                                response = request.execute()
                                
                                for item in response.get('items', []):
                                    c_snip = item['snippet']['topLevelComment']['snippet']
                                    single_yt_data.append({
                                        'ì˜ìƒì œëª©': v_title, 'ì‘ì„±ì': c_snip['authorDisplayName'],
                                        'ëŒ“ê¸€ë‚´ìš©': c_snip['textDisplay'], 'ì¢‹ì•„ìš”': c_snip['likeCount'],
                                        'ì‘ì„±ì¼': c_snip['publishedAt'][:10]
                                    })
                                    comments_collected += 1
                                    
                                next_page_token = response.get('nextPageToken')
                                if not next_page_token or comments_collected >= max_comments_single:
                                    break
                            
                            status_box.update(label="ìˆ˜ì§‘ ì™„ë£Œ!", state="complete")
                            
                            if single_yt_data:
                                df_single = pd.DataFrame(single_yt_data)
                                st.success(f"ì´ {len(df_single)}ê°œì˜ ëŒ“ê¸€ì„ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")
                                st.dataframe(df_single)
                                st.download_button("ê²°ê³¼ ë‹¤ìš´ë¡œë“œ", df_single.to_csv(index=False).encode('utf-8-sig'), f"yt_single_{video_id}.csv")
                            else:
                                st.warning("ëŒ“ê¸€ì´ ì—†ê±°ë‚˜ ëŒ“ê¸€ì´ ì¤‘ì§€ëœ ì˜ìƒì…ë‹ˆë‹¤.")
                                
                    except Exception as e:
                        status_box.update(label="ì—ëŸ¬ ë°œìƒ", state="error")
                        st.error(f"ì˜¤ë¥˜ ë‚´ìš©: {e}")

# =========================================================
# [SECTION 3] 4chan (í¬ì±ˆ) - í•´ì™¸ ì½”ì–´ ê²Œì´ë¨¸ ë°˜ì‘
# =========================================================
elif menu == "4chan (í•´ì™¸ í¬ëŸ¼)": 
    st.subheader("ğŸ€ 4chan (/v/ - Video Games) ì‹¤ì‹œê°„ ë°˜ì‘")
    st.caption("API Key ì—†ì´ í•´ì™¸ í•˜ë“œì½”ì–´ ê²Œì´ë¨¸ë“¤ì˜ 'ë‚ ê²ƒ' ë°˜ì‘ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")
    
    col1, col2 = st.columns([3, 1])
    with col1:
        # 4chanì€ ê²€ìƒ‰ APIê°€ ë”°ë¡œ ì—†ì–´ì„œ, ì „ì²´ ì¹´íƒˆë¡œê·¸ë¥¼ ê°€ì ¸ì™€ì„œ í•„í„°ë§í•´ì•¼ í•©ë‹ˆë‹¤.
        search_keyword = st.text_input("ê²€ìƒ‰ì–´ (ì˜ì–´, ì˜ˆ: Elden Ring)", value="Elden Ring")
    with col2:
        result_limit = st.number_input("ê°€ì ¸ì˜¬ ìŠ¤ë ˆë“œ ìˆ˜", min_value=1, max_value=20, value=3)

    st.info("â€» ì°¸ê³ : 4chanì€ ìµëª… ì‚¬ì´íŠ¸ íŠ¹ì„±ìƒ ê±°ì¹œ í‘œí˜„ì´ë‚˜ ë¹„ì†ì–´ê°€ í¬í•¨ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    if st.button("4chan ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘", key="btn_4chan"):
        status_box = st.status("4chan /v/ ê²Œì‹œíŒì„ ìŠ¤ìº” ì¤‘ì…ë‹ˆë‹¤...", expanded=True)
        fourchan_data = []
        
        try:
            # 1. /v/ (Video Games) ê²Œì‹œíŒì˜ ì „ì²´ ëª©ë¡(Catalog) ê°€ì ¸ì˜¤ê¸°
            # ê³µì‹ JSON API (ì¸ì¦ ë¶ˆí•„ìš”)
            catalog_url = "https://a.4cdn.org/v/catalog.json"
            res = requests.get(catalog_url, verify=False)
            
            if res.status_code == 200:
                pages = res.json()
                found_threads = []
                
                # 2. í‚¤ì›Œë“œê°€ í¬í•¨ëœ ìŠ¤ë ˆë“œ ì°¾ê¸° (ì œëª© or ë³¸ë¬¸ ê²€ìƒ‰)
                status_box.write(f"ğŸ” í˜„ì¬ í™œì„±í™”ëœ ëª¨ë“  ìŠ¤ë ˆë“œì—ì„œ '{search_keyword}' ê²€ìƒ‰ ì¤‘...")
                
                for page in pages:
                    for thread in page.get('threads', []):
                        # ì œëª©(sub)ì´ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´, ë‚´ìš©(com)ì´ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ ì²˜ë¦¬
                        title = thread.get('sub', '') 
                        comment = thread.get('com', '')
                        
                        # ëŒ€ì†Œë¬¸ì ë¬´ì‹œí•˜ê³  ê²€ìƒ‰
                        if search_keyword.lower() in title.lower() or search_keyword.lower() in comment.lower():
                            found_threads.append(thread['no']) # ìŠ¤ë ˆë“œ ë²ˆí˜¸ ì €ì¥
                            if len(found_threads) >= result_limit:
                                break
                    if len(found_threads) >= result_limit:
                        break
                
                if not found_threads:
                    status_box.update(label="ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. (í˜„ì¬ í™œì„±í™”ëœ ìŠ¤ë ˆë“œê°€ ì—†ìŒ)", state="error")
                else:
                    status_box.write(f"âœ… {len(found_threads)}ê°œì˜ ê´€ë ¨ ìŠ¤ë ˆë“œ ë°œê²¬! ìƒì„¸ ë‚´ìš©ì„ ê¸ì–´ì˜µë‹ˆë‹¤...")
                    
                    # 3. ê° ìŠ¤ë ˆë“œì˜ ëŒ“ê¸€ ìƒì„¸ ìˆ˜ì§‘
                    progress_bar = st.progress(0)
                    
                    for idx, thread_id in enumerate(found_threads):
                        thread_url = f"https://a.4cdn.org/v/thread/{thread_id}.json"
                        t_res = requests.get(thread_url, verify=False)
                        
                        if t_res.status_code == 200:
                            posts = t_res.json().get('posts', [])
                            
                            # ì²« ë²ˆì§¸ ê¸€(OP) ì •ë³´
                            op_post = posts[0]
                            op_title = op_post.get('sub', 'No Title')
                            # HTML íƒœê·¸ ì œê±° ë° í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
                            op_content = BeautifulSoup(op_post.get('com', ''), "html.parser").get_text()
                            
                            # ì›ê¸€ ì €ì¥
                            fourchan_data.append({
                                'êµ¬ë¶„': 'ì›ê¸€(Thread)',
                                'ê¸€ë²ˆí˜¸': thread_id,
                                'ì œëª©/ìš”ì•½': op_title,
                                'ì‘ì„±ì¼': datetime.fromtimestamp(op_post['time']).strftime('%Y-%m-%d %H:%M'),
                                'ë‚´ìš©': op_content,
                                'ì´ë¯¸ì§€': f"https://i.4cdn.org/v/{op_post['tim']}{op_post['ext']}" if 'tim' in op_post else None
                            })
                            
                            # ëŒ“ê¸€ë“¤(Replies) ì €ì¥
                            for reply in posts[1:]:
                                reply_content = BeautifulSoup(reply.get('com', ''), "html.parser").get_text()
                                fourchan_data.append({
                                    'êµ¬ë¶„': 'ëŒ“ê¸€(Reply)',
                                    'ê¸€ë²ˆí˜¸': thread_id,
                                    'ì œëª©/ìš”ì•½': '-', 
                                    'ì‘ì„±ì¼': datetime.fromtimestamp(reply['time']).strftime('%Y-%m-%d %H:%M'),
                                    'ë‚´ìš©': reply_content,
                                    'ì´ë¯¸ì§€': None
                                })
                        
                        time.sleep(0.5) # ì„œë²„ ë¶€í•˜ ë°©ì§€ìš© ë”œë ˆì´
                        progress_bar.progress((idx + 1) / len(found_threads))
                    
                    status_box.update(label="ìˆ˜ì§‘ ì™„ë£Œ!", state="complete")
                    
                    if fourchan_data:
                        df_4chan = pd.DataFrame(fourchan_data)
                        st.success(f"ì´ {len(df_4chan)}ê°œì˜ ë°˜ì‘ì„ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
                        st.dataframe(df_4chan)
                        st.download_button("ì—‘ì…€ ë‹¤ìš´ë¡œë“œ", df_4chan.to_csv(index=False).encode('utf-8-sig'), f"4chan_{search_keyword}.csv")
            else:
                st.error("4chan ì„œë²„ ì ‘ì†ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                
        except Exception as e:
            st.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")

# =========================================================
# [SECTION 4] DC Inside (ë””ì‹œì¸ì‚¬ì´ë“œ) - í•œêµ­ ì½”ì–´ ì»¤ë®¤ë‹ˆí‹°
# =========================================================
elif menu == "ë””ì‹œì¸ì‚¬ì´ë“œ":
    st.subheader("ğŸ”µ DC Inside ê°¤ëŸ¬ë¦¬ ìˆ˜ì§‘")
    st.caption("êµ­ë‚´ ìµœëŒ€ ì»¤ë®¤ë‹ˆí‹°ì˜ íŠ¹ì • ê°¤ëŸ¬ë¦¬ ë°˜ì‘ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤. (ê²€ìƒ‰ì–´ í¬í•¨)")

    # 1. ì„¤ì • ì…ë ¥ (2ë‹¨ ì»¬ëŸ¼)
    col1, col2 = st.columns(2)
    with col1:
        # ê°¤ëŸ¬ë¦¬ IDëŠ” URLì—ì„œ ?id= ë’¤ì— ì˜¤ëŠ” ê°’ì…ë‹ˆë‹¤.
        gallery_id = st.text_input("ê°¤ëŸ¬ë¦¬ ID (ì˜ˆ: indiegame, aoegame)", value="indiegame")
        is_minor = st.checkbox("ë§ˆì´ë„ˆ ê°¤ëŸ¬ë¦¬ ì—¬ë¶€", value=True, help="ì²´í¬ ì‹œ 'ë§ˆì´ë„ˆ ê°¤ëŸ¬ë¦¬' ì£¼ì†Œë¡œ íƒìƒ‰í•©ë‹ˆë‹¤. (ëŒ€ë¶€ë¶„ì˜ ê²Œì„ ê°¤ëŸ¬ë¦¬ëŠ” ë§ˆì´ë„ˆì…ë‹ˆë‹¤.)")
    with col2:
        keyword = st.text_input("ê²€ìƒ‰ì–´ (ì˜µì…˜, ë¹„ì›Œë‘ë©´ ì „ì²´ ìˆ˜ì§‘)", value="")
        pages_to_crawl = st.number_input("ìˆ˜ì§‘í•  í˜ì´ì§€ ìˆ˜", min_value=1, max_value=20, value=3)

    st.info("ğŸ’¡ íŒ: ê°¤ëŸ¬ë¦¬ IDëŠ” ì£¼ì†Œì°½ì˜ `id=xxxxx` ë¶€ë¶„ì…ë‹ˆë‹¤. (ì˜ˆ: `.../lists/?id=indiegame` -> `indiegame`)")

    if st.button("ë””ì‹œì¸ì‚¬ì´ë“œ ìˆ˜ì§‘ ì‹œì‘", key="btn_dc"):
        dc_data = []
        status_box = st.status("ê°¤ëŸ¬ë¦¬ì— ì ‘ì† ì¤‘ì…ë‹ˆë‹¤...", expanded=True)
        
        # ì£¼ì†Œ ê²°ì • (ë§ˆì´ë„ˆ ê°¤ëŸ¬ë¦¬ vs ì •ì‹ ê°¤ëŸ¬ë¦¬)
        base_url = "https://gall.dcinside.com/mgallery/board/lists/" if is_minor else "https://gall.dcinside.com/board/lists/"
        
        headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
        'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
        'Referer': 'https://gall.dcinside.com/',
        'Connection': 'keep-alive'
    }

        try:
            progress_bar = st.progress(0)
            
            for i in range(pages_to_crawl):
                page_num = i + 1
                
                # íŒŒë¼ë¯¸í„° ì„¤ì •
                params = {'id': gallery_id, 'page': page_num}
                if keyword:
                    params['s_type'] = 'search_subject_memo' # ì œëª©+ë‚´ìš© ê²€ìƒ‰
                    params['s_keyword'] = keyword

                status_box.write(f"ğŸ“„ {page_num}í˜ì´ì§€ ì½ëŠ” ì¤‘...")
                
                res = requests.get(base_url, headers=headers, params=params)
                
                if res.status_code != 200:
                    st.error(f"í˜ì´ì§€ ì ‘ì† ì‹¤íŒ¨ (ì½”ë“œ: {res.status_code}) - ê°¤ëŸ¬ë¦¬ IDë‚˜ ë§ˆì´ë„ˆ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
                    break
                
                soup = BeautifulSoup(res.text, 'html.parser')
                
                # ê²Œì‹œê¸€ ë¦¬ìŠ¤íŠ¸ í–‰(tr) ì°¾ê¸° (ë””ì‹œ í´ë˜ìŠ¤ êµ¬ì¡°: .ub-content)
                rows = soup.find_all('tr', class_='ub-content')
                
                if not rows:
                    status_box.warning(f"{page_num}í˜ì´ì§€ì—ì„œ ê¸€ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ë§ˆì§€ë§‰ í˜ì´ì§€ê±°ë‚˜ ê°¤ëŸ¬ë¦¬ ID ì˜¤ë¥˜)")
                    break

                for row in rows:
                    try:
                        # ê³µì§€ì‚¬í•­/ì„¤ë¬¸ ì œì™¸
                        if 'ub-notice' in row.get('class', []): continue
                        
                        # ë°ì´í„° ì¶”ì¶œ
                        title_tag = row.find('td', class_='gall_tit').find('a')
                        title = title_tag.text.strip()
                        link = "https://gall.dcinside.com" + title_tag['href']
                        
                        writer_tag = row.find('td', class_='gall_writer')
                        writer = writer_tag.get('data-nick', 'ã…‡ã…‡')
                        
                        date = row.find('td', class_='gall_date').text.strip()
                        views = row.find('td', class_='gall_count').text.strip()
                        recommend = row.find('td', class_='gall_recommend').text.strip()
                        
                        dc_data.append({
                            'ê°¤ëŸ¬ë¦¬ID': gallery_id,
                            'ì œëª©': title,
                            'ì‘ì„±ì': writer,
                            'ë‚ ì§œ': date,
                            'ì¡°íšŒìˆ˜': views,
                            'ì¶”ì²œìˆ˜': recommend,
                            'ë§í¬': link
                        })
                    except Exception as e:
                        continue # íŒŒì‹± ì—ëŸ¬ ë‚œ í–‰ì€ ê±´ë„ˆëœ€
                
                time.sleep(0.5) # ì„œë²„ ë¶€í•˜ ë°©ì§€ ë”œë ˆì´
                progress_bar.progress((i + 1) / pages_to_crawl)

            status_box.update(label="ìˆ˜ì§‘ ì™„ë£Œ!", state="complete")
            
            if dc_data:
                df_dc = pd.DataFrame(dc_data)
                st.success(f"ì´ {len(df_dc)}ê°œì˜ ê²Œì‹œê¸€ì„ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
                st.dataframe(df_dc)
                
                # íŒŒì¼ëª… ìƒì„±
                csv_name = f"dc_{gallery_id}_{keyword}.csv" if keyword else f"dc_{gallery_id}_recent.csv"
                st.download_button("ì—‘ì…€ ë‹¤ìš´ë¡œë“œ", df_dc.to_csv(index=False).encode('utf-8-sig'), csv_name)
            else:
                st.warning("ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ê°¤ëŸ¬ë¦¬ IDë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

        except Exception as e:
            st.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")