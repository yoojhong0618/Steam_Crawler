import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup
from googleapiclient.discovery import build
from datetime import datetime, time
import time as time_lib

# ==========================================
# 1. í˜ì´ì§€ ì„¤ì •
# ==========================================
st.set_page_config(page_title="Game Community Crawler", layout="wide")

# ==========================================
# 2. í¬ë¡¤ë§ í•¨ìˆ˜ ì •ì˜
# ==========================================

def get_steam_reviews(app_id, num_reviews=100):
    """Steam App IDë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¦¬ë·°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
    url = f"https://store.steampowered.com/appreviews/{app_id}"
    reviews = []
    cursor = '*'
    
    params = {
        'json': 1,
        'filter': 'updated', # ìµœê·¼ ìˆ˜ì •ëœ ìˆœ
        'language': 'english', # í•„ìš”ì‹œ 'all' ë˜ëŠ” 'koreana'ë¡œ ë³€ê²½ ê°€ëŠ¥
        'day_range': 9223372036854775807,
        'review_type': 'all',
        'purchase_type': 'all',
        'num_per_page': 100
    }

    try:
        while len(reviews) < num_reviews:
            params['cursor'] = cursor
            response = requests.get(url, params=params)
            
            if response.status_code != 200:
                st.error(f"Steam API ì˜¤ë¥˜: ìƒíƒœ ì½”ë“œ {response.status_code}")
                break
                
            data = response.json()
            
            if 'reviews' in data and len(data['reviews']) > 0:
                for r in data['reviews']:
                    reviews.append({
                        'Author_ID': r['author']['steamid'],
                        'Playtime_Forever': r['author']['playtime_forever'],
                        'Review_Text': r['review'],
                        'Voted_Up': r['voted_up'],
                        'Votes_Up': r['votes_up'],
                        'Date_Posted': datetime.fromtimestamp(r['timestamp_created']).strftime('%Y-%m-%d')
                    })
                cursor = data['cursor']
            else:
                break
    except Exception as e:
        st.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
            
    return pd.DataFrame(reviews[:num_reviews])

def get_steam_discussions(app_id):
    """Steam í† ë¡ ì¥(General)ì˜ 1í˜ì´ì§€ ê²Œì‹œê¸€ ëª©ë¡ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
    discussions = []
    # ê¸°ë³¸ í† ë¡ ì¥ URL êµ¬ì¡°
    target_url = f"https://steamcommunity.com/app/{app_id}/discussions/"
    
    try:
        res = requests.get(target_url)
        if res.status_code != 200:
            st.error("í† ë¡ ì¥ í˜ì´ì§€ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. App IDë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
            return pd.DataFrame()

        soup = BeautifulSoup(res.text, 'html.parser')
        rows = soup.find_all('div', class_='forum_topic')
        
        for row in rows:
            topic = row.find('div', class_='forum_topic_name')
            author = row.find('div', class_='forum_topic_op')
            reply_count = row.find('div', class_='forum_topic_reply_count')
            
            if topic:
                title_text = topic.get_text(strip=True)
                link = topic.find('a')['href']
                
                discussions.append({
                    "Title": title_text,
                    "Author": author.get_text(strip=True) if author else "Unknown",
                    "Replies": reply_count.get_text(strip=True) if reply_count else "0",
                    "Link": link
                })
    except Exception as e:
        st.error(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
            
    return pd.DataFrame(discussions)

def get_youtube_videos(api_key, query, start, end, max_results):
    """ê¸°ê°„ ë‚´ Youtube ì˜ìƒ ê²€ìƒ‰"""
    youtube = build('youtube', 'v3', developerKey=api_key)
    
    start_dt = datetime.combine(start, time.min).isoformat() + "Z"
    end_dt = datetime.combine(end, time.max).isoformat() + "Z"
    
    video_list = []
    try:
        search_response = youtube.search().list(
            q=query,
            type="video",
            part="id,snippet",
            order="viewCount",
            publishedAfter=start_dt,
            publishedBefore=end_dt,
            maxResults=max_results
        ).execute()

        for item in search_response.get("items", []):
            video_list.append({
                "video_id": item["id"]["videoId"],
                "title": item["snippet"]["title"],
                "published_at": item["snippet"]["publishedAt"],
                "channel": item["snippet"]["channelTitle"]
            })
    except Exception as e:
        st.error(f"YouTube ê²€ìƒ‰ ì˜¤ë¥˜ (API Keyë¥¼ í™•ì¸í•˜ì„¸ìš”): {e}")
        
    return video_list

def get_youtube_comments(api_key, video_id, max_comments):
    """ì˜ìƒ ëŒ“ê¸€ ìˆ˜ì§‘ (ì¸ê¸°ìˆœ)"""
    youtube = build('youtube', 'v3', developerKey=api_key)
    comments = []
    
    try:
        request = youtube.commentThreads().list(
            part="snippet",
            videoId=video_id,
            maxResults=min(max_comments, 100),
            textFormat="plainText",
            order="relevance" 
        )
        
        while request and len(comments) < max_comments:
            response = request.execute()
            for item in response['items']:
                comment_snip = item['snippet']['topLevelComment']['snippet']
                comments.append({
                    "Author": comment_snip['authorDisplayName'],
                    "Comment": comment_snip['textDisplay'],
                    "Likes": comment_snip['likeCount'],
                    "Date": comment_snip['publishedAt']
                })
            
            if 'nextPageToken' in response and len(comments) < max_comments:
                request = youtube.commentThreads().list_next(request, response)
            else:
                break
    except:
        pass
        
    return comments

# ==========================================
# 3. ì‚¬ì´ë“œë°” ë©”ë‰´ êµ¬ì„±
# ==========================================
st.sidebar.title("Navigation")
menu = st.sidebar.radio("ë°ì´í„° ì†ŒìŠ¤ ì„ íƒ", ["Steam Reviews", "Steam Discussions", "Youtube Crawler", "Reddit (ì¤€ë¹„ì¤‘)"])

st.title("í†µí•© ê²Œì„ ì—¬ë¡  ë¶„ì„ê¸°")

# ==========================================
# 4. ë©”ë‰´ë³„ ë©”ì¸ í™”ë©´ ë¡œì§
# ==========================================

# --- [1] Steam Reviews ---
if menu == "Steam Reviews":
    st.header("ğŸŸ¦ Steam Review Crawler")
    st.markdown("íŠ¹ì • ê²Œì„ì˜ **App ID**ë¥¼ ì…ë ¥í•˜ì—¬ ë¦¬ë·°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")
    
    col1, col2 = st.columns([1, 2])
    with col1:
        app_id = st.text_input("Steam App ID", value="1245620", help="ìƒì  í˜ì´ì§€ URLì˜ ìˆ«ì ë¶€ë¶„ì…ë‹ˆë‹¤. ì˜ˆ: Elden Ring = 1245620")
    with col2:
        num_reviews = st.slider("ìˆ˜ì§‘í•  ë¦¬ë·° ìˆ˜ (ìµœì‹ ìˆœ)", 50, 2000, 100, step=50)

    if st.button("ë¦¬ë·° ìˆ˜ì§‘ ì‹œì‘"):
        if not app_id.isdigit():
            st.error("App IDëŠ” ìˆ«ìì—¬ì•¼ í•©ë‹ˆë‹¤.")
        else:
            with st.spinner(f"App ID {app_id}ì˜ ë¦¬ë·°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘..."):
                df = get_steam_reviews(app_id, num_reviews)
                if not df.empty:
                    st.success(f"ì´ {len(df)}ê°œì˜ ë¦¬ë·° ìˆ˜ì§‘ ì™„ë£Œ!")
                    st.dataframe(df)
                    
                    csv = df.to_csv(index=False).encode('utf-8-sig')
                    st.download_button("CSV ë‹¤ìš´ë¡œë“œ", csv, f"steam_reviews_{app_id}.csv", "text/csv")
                else:
                    st.warning("ë¦¬ë·°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. App IDë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

# --- [2] Steam Discussions ---
elif menu == "Steam Discussions":
    st.header("ğŸ’¬ Steam Discussion Crawler")
    st.markdown("íŠ¹ì • ê²Œì„ì˜ **í† ë¡ ì¥(General)** ê¸€ ëª©ë¡ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")
    
    app_id_disc = st.text_input("Steam App ID", value="1245620", help="ìƒì  í˜ì´ì§€ URLì˜ ìˆ«ì ë¶€ë¶„ì…ë‹ˆë‹¤.")
    
    if st.button("í† ë¡ ì¥ ê¸€ ëª©ë¡ ìˆ˜ì§‘"):
        if not app_id_disc.isdigit():
            st.error("App IDëŠ” ìˆ«ìì—¬ì•¼ í•©ë‹ˆë‹¤.")
        else:
            with st.spinner("í† ë¡ ì¥ì„ ê²€ìƒ‰ ì¤‘ì…ë‹ˆë‹¤..."):
                df_disc = get_steam_discussions(app_id_disc)
                if not df_disc.empty:
                    st.success(f"í˜„ì¬ í˜ì´ì§€ì˜ í† ë¡  ê¸€ {len(df_disc)}ê°œë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")
                    st.dataframe(df_disc)
                    
                    # ë§í¬ í´ë¦­ ê°€ëŠ¥í•˜ê²Œ í‘œì‹œ
                    for index, row in df_disc.iterrows():
                        st.markdown(f"**[{row['Title']}]({row['Link']})** (ëŒ“ê¸€: {row['Replies']})")
                        
                    csv_disc = df_disc.to_csv(index=False).encode('utf-8-sig')
                    st.download_button("CSV ë‹¤ìš´ë¡œë“œ", csv_disc, f"steam_discussion_{app_id_disc}.csv", "text/csv")
                else:
                    st.warning("ê²Œì‹œê¸€ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. App IDê°€ ì •í™•í•œì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")

# --- [3] Youtube Crawler ---
elif menu == "Youtube Crawler":
    st.header("ğŸŸ¥ Youtube Comment Crawler")
    
    # API í‚¤ ì…ë ¥ (ì´ íƒ­ì—ì„œë§Œ ë³´ì„)
    api_key_input = st.text_input("YouTube Data API Key", type="password")
    
    st.markdown("---")
    
    col_search, col_count = st.columns([3, 1])
    with col_search:
        yt_query = st.text_input("ê²€ìƒ‰ì–´ (ê²Œì„ ì´ë¦„)", "League of Legends")
    with col_count:
        max_vids = st.number_input("ì˜ìƒ ê°œìˆ˜ ì œí•œ", min_value=1, max_value=50, value=5)

    col_start, col_end = st.columns(2)
    with col_start:
        start_date = st.date_input("ì‹œì‘ ë‚ ì§œ", value=datetime(2024, 1, 1))
    with col_end:
        end_date = st.date_input("ì¢…ë£Œ ë‚ ì§œ", value=datetime.now())

    if st.button("YouTube ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘"):
        if not api_key_input:
            st.error("API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        else:
            with st.status("ë°ì´í„° ìˆ˜ì§‘ ì§„í–‰ ì¤‘...", expanded=True) as status:
                st.write("ğŸ” ì˜ìƒì„ ê²€ìƒ‰í•©ë‹ˆë‹¤...")
                videos = get_youtube_videos(api_key_input, yt_query, start_date, end_date, max_vids)
                
                if not videos:
                    status.update(label="í•´ë‹¹ ê¸°ê°„ì— ê²€ìƒ‰ëœ ì˜ìƒì´ ì—†ìŠµë‹ˆë‹¤.", state="error")
                else:
                    st.write(f"ì´ {len(videos)}ê°œì˜ ì˜ìƒì„ ì°¾ì•˜ìŠµë‹ˆë‹¤. ëŒ“ê¸€ ìˆ˜ì§‘ ì‹œì‘ (ì¸ê¸°ìˆœ)...")
                    
                    all_yt_data = []
                    prog_bar = st.progress(0)
                    
                    for idx, video in enumerate(videos):
                        prog_bar.progress((idx + 1) / len(videos))
                        st.write(f"Collecting: {video['title'][:30]}...")
                        
                        # ì˜ìƒë‹¹ ëŒ“ê¸€ ìµœëŒ€ 50ê°œ (ì¡°ì ˆ ê°€ëŠ¥)
                        comments = get_youtube_comments(api_key_input, video['video_id'], 50)
                        
                        for c in comments:
                            all_yt_data.append({
                                "Video_Title": video['title'],
                                "Video_Publish_Date": video['published_at'],
                                "Video_Channel": video['channel'],
                                "Comment_Author": c['Author'],
                                "Comment_Text": c['Comment'],
                                "Comment_Likes": c['Likes'],
                                "Comment_Date": c['Date']
                            })
                        time_lib.sleep(0.1) 
                    
                    status.update(label="ìˆ˜ì§‘ ì™„ë£Œ!", state="complete")
                    
                    if all_yt_data:
                        df_yt = pd.DataFrame(all_yt_data)
                        st.success(f"ì´ {len(df_yt)}ê°œì˜ ëŒ“ê¸€ ìˆ˜ì§‘ ì™„ë£Œ.")
                        st.dataframe(df_yt.head())
                        
                        csv_yt = df_yt.to_csv(index=False).encode('utf-8-sig')
                        st.download_button(
                            label="CSV ë‹¤ìš´ë¡œë“œ",
                            data=csv_yt,
                            file_name=f"youtube_{yt_query}.csv",
                            mime="text/csv"
                        )
                    else:
                        st.warning("ëŒ“ê¸€ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

# --- [4] Reddit (Placeholder) ---
elif menu == "Reddit (ì¤€ë¹„ì¤‘)":
    st.header("ğŸŸ§ Reddit Crawler")
    st.info("ì´ ê¸°ëŠ¥ì€ í˜„ì¬ ê°œë°œ ì¤‘ì…ë‹ˆë‹¤. (Reddit API ì—°ë™ ì˜ˆì •)")