import streamlit as st
import requests
import pandas as pd
import time
import random  # ëœë¤ ë”œë ˆì´ìš©
from datetime import datetime
from bs4 import BeautifulSoup

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ìŠ¤íŒ€ ë¦¬ë·° & í† ë¡  ìˆ˜ì§‘ê¸°", layout="wide")

# --- ğŸ” ë¹„ë°€ë²ˆí˜¸ ì ê¸ˆ ---
password = st.text_input("ğŸ”’ ì ‘ì† ì•”í˜¸", type="password")
if password != "smilegate":
    st.warning("ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤.")
    st.stop()

st.title("Steam ë¦¬ë·° & í† ë¡  ìˆ˜ì§‘ê¸° (Debug Ver.)")

# --- ì‚¬ì´ë“œë°” ---
with st.sidebar:
    st.header("ì„¤ì •")
    menu = st.selectbox("ë¶„ì„ ì±„ë„", ["Steam (ìŠ¤íŒ€)", "Reddit (ì¤€ë¹„ì¤‘)", "YouTube (ì¤€ë¹„ì¤‘)"])
    st.divider()

if menu == "Steam (ìŠ¤íŒ€)":
    tab1, tab2 = st.tabs(["â­ ë¦¬ë·° ìˆ˜ì§‘ (ì •ìƒ)", "ğŸ—£ï¸ í† ë¡ ì¥ ìˆ˜ì§‘ (ë””ë²„ê¹…)"])
    
    # [TAB 1] ë¦¬ë·° ìˆ˜ì§‘ (ê¸°ì¡´ ê¸°ëŠ¥ ìœ ì§€)
    with tab1:
        st.subheader("â­ ìŠ¤íŒ€ ë¦¬ë·° ìˆ˜ì§‘ (API ë°©ì‹)")
        col1, col2 = st.columns(2)
        with col1:
            app_id_review = st.text_input("App ID (ë¦¬ë·°ìš©)", value="1562700")
        with col2:
            language = st.selectbox("ì–¸ì–´", ["all", "koreana", "english", "japanese", "schinese"], index=0)
        
        start_date = st.date_input("ì–¸ì œë¶€í„° ìˆ˜ì§‘í• ê¹Œìš”?", datetime(2025, 2, 1))
        
        if st.button("ë¦¬ë·° ìˆ˜ì§‘ ì‹œì‘ ğŸš€", key="btn_review"):
            st.toast("ë¦¬ë·° íƒìƒ‰ ì‹œì‘...")
            all_reviews = []
            cursor = '*'
            progress_bar = st.progress(0)
            status_box = st.info(f"íƒìƒ‰ ì¤‘... (ëª©í‘œ: {start_date})")
            
            try:
                for i in range(5000):
                    params = {
                        'json': 1, 'cursor': cursor, 'language': language,
                        'num_per_page': 100, 'purchase_type': 'all', 'filter': 'recent'
                    }
                    res = requests.get(f"https://store.steampowered.com/appreviews/{app_id_review}", params=params)
                    data = res.json()
                    
                    if 'reviews' in data and len(data['reviews']) > 0:
                        last_ts = data['reviews'][-1]['timestamp_created']
                        curr_date = pd.to_datetime(last_ts, unit='s').date()
                        for r in data['reviews']:
                            r_date = pd.to_datetime(r['timestamp_created'], unit='s').date()
                            if r_date >= start_date:
                                all_reviews.append({
                                    'ì‘ì„±ì¼': r_date, 
                                    'ë‚´ìš©': r['review'].replace('\n', ' '), 
                                    'ì¶”ì²œìˆ˜': r['votes_up']
                                })
                        cursor = data['cursor']
                        status_box.info(f"{len(all_reviews)}ê°œ ìˆ˜ì§‘ ì¤‘... (í˜„ì¬: {curr_date})")
                        if curr_date < start_date: break
                    else: break
                
                if all_reviews:
                    df = pd.DataFrame(all_reviews)
                    filtered_df = df[df['ì‘ì„±ì¼'] >= start_date]
                    st.success(f"âœ… ì™„ë£Œ! {len(filtered_df)}ê°œ ìˆ˜ì§‘ë¨.")
                    st.dataframe(filtered_df)
                    st.download_button("ì—‘ì…€ ë‹¤ìš´ë¡œë“œ", filtered_df.to_csv(index=False).encode('utf-8-sig'), "steam_reviews.csv")
                else:
                    st.warning("ìˆ˜ì§‘ëœ ë¦¬ë·°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            except Exception as e:
                st.error(f"ì˜¤ë¥˜: {e}")

    # [TAB 2] í† ë¡ ì¥ ìˆ˜ì§‘ (ë””ë²„ê¹… ê¸°ëŠ¥ ëŒ€í­ ê°•í™” ğŸ› ï¸)
    with tab2:
        st.subheader("ğŸ—£ï¸ í† ë¡ ì¥ ìƒì„¸ ìˆ˜ì§‘ (ê°•ë ¥ ë””ë²„ê¹…)")
        
        target_url = st.text_input(
            "ìˆ˜ì§‘í•  í† ë¡ ì¥ URL", 
            value="https://steamcommunity.com/app/1562700/discussions/"
        )
        
        pages_to_crawl = st.number_input("íƒìƒ‰ í˜ì´ì§€ ìˆ˜", min_value=1, max_value=50, value=3)
        
        if st.button("í† ë¡ ê¸€ ìˆ˜ì§‘ ì‹œì‘ (ì§„ë‹¨ ëª¨ë“œ)", key="btn_discuss"):
            st.toast("ì„œë²„ì— ì ‘ì†ì„ ì‹œë„í•©ë‹ˆë‹¤...")
            discussion_data = []
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            # 1. í—¤ë” ê°•í™” (ì¼ë°˜ ë¸Œë¼ìš°ì €ì¸ ì²™ ìœ„ì¥)
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.9,ko;q=0.8',
                'Referer': 'https://store.steampowered.com/'
            }
            # 2. í•„ìˆ˜ ì¿ í‚¤
            cookies = {'wants_mature_content': '1', 'birthtime': '660000001', 'lastagecheckage': '1-January-1990'}
            
            try:
                if not target_url.endswith('/') and '?' not in target_url:
                    target_url += '/'

                for p in range(pages_to_crawl):
                    full_url = f"{target_url}?fp={p+1}"
                    
                    # 3. ëœë¤ ë”œë ˆì´ (1ì´ˆ ~ 3ì´ˆ ì‚¬ì´) - ìš”ì²­í•˜ì‹  ë”œë ˆì´ ê°•í™”
                    sleep_time = random.uniform(1.5, 3.5)
                    time.sleep(sleep_time)
                    
                    # ì ‘ì† ì‹œë„
                    res = requests.get(full_url, headers=headers, cookies=cookies, timeout=15)
                    
                    # 4. [ì¤‘ìš”] ìƒíƒœ ì½”ë“œ í™•ì¸ (200ì´ ì•„ë‹ˆë©´ ì°¨ë‹¨/ì—ëŸ¬)
                    if res.status_code != 200:
                        st.error(f"âŒ ì ‘ì† ì‹¤íŒ¨! ìƒíƒœ ì½”ë“œ: {res.status_code}")
                        st.write("ì„œë²„ê°€ ìš”ì²­ì„ ê±°ë¶€í–ˆìŠµë‹ˆë‹¤.")
                        break

                    soup = BeautifulSoup(res.text, 'html.parser')
                    topics = soup.find_all('a', class_='forum_topic_link')
                    
                    # 5. [ì¤‘ìš”] ê¸€ì„ ëª» ì°¾ì•˜ì„ ë•Œ -> HTML ê¹Œë³´ê¸° (ë””ë²„ê¹…)
                    if len(topics) == 0:
                        st.warning(f"âš ï¸ {p+1}í˜ì´ì§€ì—ì„œ ê¸€ ëª©ë¡ì´ 0ê°œì…ë‹ˆë‹¤.")
                        
                        with st.expander("ğŸ”´ ê¸´ê¸‰ ë””ë²„ê¹…: ë¡œë´‡ì´ ë³¸ í™”ë©´ (í´ë¦­í•´ì„œ í™•ì¸)", expanded=True):
                            st.write(f"**ì ‘ì† URL:** {full_url}")
                            st.write(f"**ì‘ë‹µ ì½”ë“œ:** {res.status_code} (200ì´ë©´ ì ‘ì†ì€ ì„±ê³µ)")
                            
                            # í˜ì´ì§€ ì œëª© í™•ì¸
                            page_title = soup.title.string.strip() if soup.title else "ì œëª© ì—†ìŒ"
                            st.write(f"**í˜ì´ì§€ ì œëª©:** {page_title}")
                            
                            # HTML ë‚´ìš© ì•ë¶€ë¶„ ì¶œë ¥ (ì—¬ê¸°ì— 'Blocked'ë‚˜ 'Age Gate' ë“±ì´ ìˆëŠ”ì§€ í™•ì¸)
                            st.code(soup.prettify()[:2000], language='html')
                            
                            if "General Discussions" in page_title and len(topics) == 0:
                                st.error("ë¶„ì„: ì œëª©ì€ ë§ëŠ”ë° ëª©ë¡ì´ ì—†ë‹¤? -> 100% ìŠ¤íŒ€ì´ ë¦¬ìŠ¤íŠ¸ë¥¼ ìˆ¨ê¸´ ê²ë‹ˆë‹¤. (IP ì°¨ë‹¨/ë´‡ íƒì§€)")
                        break 
                    
                    status_text.text(f"âœ… {p+1}í˜ì´ì§€ ì ‘ì† ì„±ê³µ! ({len(topics)}ê°œ ê¸€ ë°œê²¬)")
                    
                    # ìƒì„¸ ë‚´ìš© ìˆ˜ì§‘
                    for idx, topic in enumerate(topics):
                        title = topic.text.strip()
                        link = topic['href']
                        
                        time.sleep(random.uniform(0.5, 1.5)) # ìƒì„¸í˜ì´ì§€ ì§„ì… ì „ ë”œë ˆì´
                        sub_res = requests.get(link, headers=headers, cookies=cookies)
                        sub_soup = BeautifulSoup(sub_res.text, 'html.parser')
                        
                        content_div = sub_soup.find('div', class_='forum_op')
                        if content_div:
                            author = content_div.find('div', class_='author').text.strip()
                            main_text = content_div.find('div', class_='content').text.strip()
                            date_posted = content_div.find('div', class_='date').text.strip()
                            
                            discussion_data.append({'Type': 'ë³¸ë¬¸', 'Title': title, 'Author': author, 'Content': main_text, 'Date': date_posted, 'Link': link})
                            
                            comments = sub_soup.find_all('div', class_='commentthread_comment')
                            for comm in comments:
                                try:
                                    c_author = comm.find('bdi').text.strip()
                                    c_text = comm.find('div', class_='commentthread_comment_text').text.strip()
                                    discussion_data.append({'Type': 'ëŒ“ê¸€', 'Title': f"(Re) {title}", 'Author': c_author, 'Content': c_text, 'Date': '-', 'Link': link})
                                except: continue
                        
                        current_progress = (p / pages_to_crawl) + ((idx + 1) / len(topics) / pages_to_crawl)
                        progress_bar.progress(min(current_progress, 0.99))

                progress_bar.progress(1.0)
                
                if discussion_data:
                    df = pd.DataFrame(discussion_data)
                    st.success(f"ğŸ‰ ìˆ˜ì§‘ ì™„ë£Œ! ì´ {len(df)}ê°œ ë°ì´í„°")
                    st.dataframe(df)
                    st.download_button("ì—‘ì…€ ë‹¤ìš´ë¡œë“œ", df.to_csv(index=False).encode('utf-8-sig'), "steam_discuss_full.csv")
                else:
                    st.error("ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    
            except Exception as e:
                st.error(f"ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ: {e}")